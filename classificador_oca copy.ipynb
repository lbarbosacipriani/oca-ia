{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beb18702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (2.3.3)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (11.3.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (2.3.3)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 5)) (1.7.2)\n",
      "Requirement already satisfied: timm in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 6)) (1.0.20)\n",
      "Requirement already satisfied: jupyter_contrib_nbextensions in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 8)) (8.1.7)\n",
      "Requirement already satisfied: widgetsnbextension in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 9)) (4.0.14)\n",
      "Collecting pandas-profiling (from -r requirements.txt (line 10))\n",
      "  Using cached pandas_profiling-3.2.0-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 11)) (3.10.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.13/site-packages (from timm->-r requirements.txt (line 6)) (0.23.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.13/site-packages (from timm->-r requirements.txt (line 6)) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.13/site-packages (from timm->-r requirements.txt (line 6)) (0.35.3)\n",
      "Requirement already satisfied: safetensors in ./.venv/lib/python3.13/site-packages (from timm->-r requirements.txt (line 6)) (0.6.2)\n",
      "Requirement already satisfied: ipython_genutils in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: jupyter_contrib_core>=0.3.3 in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (0.4.2)\n",
      "Requirement already satisfied: jupyter_core in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (5.8.1)\n",
      "Requirement already satisfied: jupyter_highlight_selected_word>=0.1.1 in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: jupyter_nbextensions_configurator>=0.4.0 in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (0.6.4)\n",
      "Requirement already satisfied: nbconvert>=6.0 in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (7.16.6)\n",
      "Requirement already satisfied: notebook>=6.0 in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (7.4.7)\n",
      "Requirement already satisfied: tornado in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=4.1 in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (5.14.3)\n",
      "Requirement already satisfied: lxml in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (6.0.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.13/site-packages (from ipywidgets->-r requirements.txt (line 8)) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.13/site-packages (from ipywidgets->-r requirements.txt (line 8)) (9.6.0)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./.venv/lib/python3.13/site-packages (from ipywidgets->-r requirements.txt (line 8)) (3.0.15)\n",
      "INFO: pip is looking at multiple versions of pandas-profiling to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached pandas_profiling-3.1.0-py2.py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached pandas_profiling-3.0.0-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pydantic>=1.8.1 (from pandas-profiling->-r requirements.txt (line 10))\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting visions==0.7.1 (from visions[type_image_path]==0.7.1->pandas-profiling->-r requirements.txt (line 10))\n",
      "  Using cached visions-0.7.1-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting htmlmin>=0.1.12 (from pandas-profiling->-r requirements.txt (line 10))\n",
      "  Using cached htmlmin-0.1.12.tar.gz (19 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[27 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/home/leo/Documents/dev/oca-ia/.venv/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/home/leo/Documents/dev/oca-ia/.venv/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m                              \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/home/leo/Documents/dev/oca-ia/.venv/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-80wb2g1d/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-80wb2g1d/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m301\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-80wb2g1d/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-80wb2g1d/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m4\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-install-fhkpfl0l/htmlmin_110e62f20b084cbea34b2c56526d87e2/htmlmin/__init__.py\"\u001b[0m, line \u001b[35m28\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     from .main import minify, Minifier\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-install-fhkpfl0l/htmlmin_110e62f20b084cbea34b2c56526d87e2/htmlmin/main.py\"\u001b[0m, line \u001b[35m28\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     import cgi\n",
      "  \u001b[31m   \u001b[0m \u001b[1;35mModuleNotFoundError\u001b[0m: \u001b[35mNo module named 'cgi'\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b40d20cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image, ImageChops\n",
    "import numpy as np\n",
    "import torch\n",
    "from lib.ImageFIlter import treat_image_PIL\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import  DataLoader, TensorDataset, Dataset\n",
    "from torch import nn\n",
    "import timm\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22170478",
   "metadata": {},
   "source": [
    "## Exemplo de classificador com leitura do CSV. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de731cfa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9688a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Cria funcao para validar se pasta a ser inserida existe. Caso nao exista, cria a pasta\n",
    "def create_folder_if_not_exists(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f'Pasta {folder_path} criada.')\n",
    "    else:\n",
    "        print(f'Pasta {folder_path} ja existe.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a724514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta output ja existe.\n",
      "Pasta models ja existe.\n"
     ]
    }
   ],
   "source": [
    "## Criacao de pastas output e models se nao existirem\n",
    "import os\n",
    "create_folder_if_not_exists('output')\n",
    "create_folder_if_not_exists('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd08e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_file = pd.read_csv('dataset/oca.csv')\n",
    "ds_file = ds_file[['path','oca']]\n",
    "ds_file.rename(columns={'oca':'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10c59064",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_file.to_csv('dataset/oca_incor.csv', index=False)\n",
    "ds_file = pd.read_csv('dataset/oca_incor.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52323b9c",
   "metadata": {},
   "source": [
    "## Geracao Tensor com imagens filtradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eaf296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop\n",
    "img_dataset = np.ones((ds_file.shape[0],3,256,256),dtype=np.uint8)\n",
    "\n",
    "j=0\n",
    "for i in ds_file['path']:\n",
    "    img_dataset[j]=treat_image_PIL('dataset/path/'+i,2)\n",
    "    j+=1\n",
    "tensor_imagem = torch.tensor(img_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966dbdca",
   "metadata": {},
   "source": [
    "## Geracao de rótulos para classificação\n",
    "\n",
    "Colunas utilizadas para classificacao (Labels)\n",
    "\n",
    "labels = [ OCA, NOCA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29f9adfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_label = torch.tensor(np.array(ds_file['label'].astype(int)))\n",
    "tensor_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bd06990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "ds_file.columns\n",
    "labels = [ 'label']\n",
    "ds_labels = ds_file[labels]\n",
    "ds_labels=ds_labels.astype(int)#,'False':0})\n",
    "tensor_label = torch.tensor(np.array(ds_file['label'].astype(int)))\n",
    "tensor_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb9fcbf",
   "metadata": {},
   "source": [
    "### Resumo:\n",
    "\n",
    "model(tensor_imagem,ds_id_sex_et)\n",
    "\n",
    "labels : tensor_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc188221",
   "metadata": {},
   "source": [
    "## Criacao Subset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fdd31e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead13f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f0dcb69",
   "metadata": {},
   "source": [
    "## K-fold = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38e96965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=2, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=2)\n",
    "kf.get_n_splits(tensor_imagem)\n",
    "print(kf)\n",
    "#for i in enumerate(kf.split(tensor_imagem)):\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64652bf",
   "metadata": {},
   "source": [
    "## Criacao do dataset de treino e validacao com batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "925d5185",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "train_dataset = TensorDataset(tensor_imagem, tensor_label)\n",
    "train_loader_img = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d682980b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba835adf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "addfb539",
   "metadata": {},
   "source": [
    "## Create a model to concat layers\n",
    "\n",
    "O modelo utilizado será o mesmo do artigo: Artificial Intelligence-Driven Screening System for Rapid Image-Based Classification of 12-Lead ECG Exams: A Promising Solution for Emergency Room Prioritization \n",
    "\n",
    "Desenvolvido conforme a figura abaixo: \n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f32068ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models import ECGClassifierResnet\n",
    "\n",
    "class ECGClassifierResnet(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(ECGClassifierResnet, self).__init__()\n",
    "        # Where we define all the parts of the model\n",
    "        #self.base_model = timm.create_model('efficientnet_b0', pretrained=True) \n",
    "        self.base_model=timm.create_model('resnet50d.ra4_e3600_r224_in1k',pretrained=True)\n",
    "        #self.base_model = timm.create_model('vit_mediumd_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k',num_classes=5,pretrained=True)\n",
    "\n",
    "\n",
    "        self.features = nn.Sequential(*list(self.base_model.children())[:-1])\n",
    "\n",
    "        enet_out_size = 2048        # Make a classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(enet_out_size,1)\n",
    "        ) # saida como Sigmoid multilabel\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Connect these parts and return the output\n",
    "        x = self.features(x)\n",
    "        output = self.classifier(x)\n",
    "        #output = nn.Softmax(dim=1)(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c98c40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c198fcb9",
   "metadata": {},
   "source": [
    "## Train - Valid Loop\n",
    "\n",
    "Fluxo de treinamento\n",
    "\n",
    "Input: modelo, dataloader_train, dataloader_teste , epocas.\n",
    "- Para cada epoca\n",
    "\n",
    "    - treinamento:\n",
    "\n",
    "        - para cada batch (lote de imagens) no dataloader_train: (executo para todo o dataloader)\n",
    "            \n",
    "            leitura das imagens e rotulos. \n",
    "            \n",
    "            prediz o rotulo (outputs): rotulos preditos para o tamanho do batch (ex: para um batch de 10, o \n",
    "            output tem tamanho 10)\n",
    "            \n",
    "            calcula o Loss () \n",
    "            \n",
    "            aplica o backward -> Ajuste dos pesos nos neuronios de acordo com o valor de loss (w_{i})\n",
    "\n",
    "            otimizo o modelo com optimizer.step()\n",
    "\n",
    "    - validacao (avaliacao do desempenho para aquele conjunto de treinamento):\n",
    "        - para cada batch (lote de imagens) no dataloader_train: (executo para todo o dataloader)\n",
    "\n",
    "            leitura das imagens e rotulos.\n",
    "\n",
    "            prediz o rotulo (outputs): rotulos preditos para o tamanho do batch (ex: para um batch de 10, o \n",
    "            output tem tamanho 10)\n",
    "            \n",
    "            calcula o Loss ()\n",
    "\n",
    "\n",
    "    - Calculo de metricas de loss media de validacao.\n",
    "\n",
    "    - Calculo de metrica de loss media de treinamento. \n",
    "\n",
    "    - Print de metricas de desempenho para a epoca. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "809e6d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_model(model, path = 'output/modelos', name_file='model.pth'):\n",
    "    full_path = os.path.join(path, name_file)\n",
    "    torch.save(model.state_dict(), full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60622c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def salvar_metricas(path, name_file_train='train_loss_total.npy', name_file_val='val_loss_total.npy',predict_label=None, true_label=None):\n",
    "    create_folder_if_not_exists(path)\n",
    "    full_path_train = os.path.join(path, name_file_train)\n",
    "    full_path_val = os.path.join(path, name_file_val)\n",
    "    np.save(full_path_train, np.array(predict_label))\n",
    "    np.save(full_path_val, np.array(true_label))\n",
    "    print(f'Metricas salvas em {path} com os nomes {name_file_train} e {name_file_val} e tamanhos {np.array(predict_label).shape} e {np.array(true_label).shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caefa905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import  Dataset\n",
    "\n",
    "class Subset(Dataset):\n",
    "    r\"\"\"\n",
    "    Subset of a dataset at specified indices.\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): The whole Dataset\n",
    "        indices (sequence): Indices in the whole set selected for subset\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    @property\n",
    "    def classes(self):\n",
    "        return self.data.classes\n",
    "\n",
    "    def shape(self):\n",
    "        return self.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5d5e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def simple_loop(model, train_image, val_image, epochs, batch_size, fold_index):\n",
    "    # Simple training loop\n",
    "    num_epochs = epochs\n",
    "    train_losses, val_losses = [], []\n",
    "    lim_loss = 1.5\n",
    "    iter_size = batch_size\n",
    "    print(f'Number of training images per iteration: {iter_size}')\n",
    "    #model = modelo( num_classes=5)\n",
    "    model.to(device)\n",
    "    criterion =  nn.L1Loss()\n",
    "    #criterion =  nn.CrossEntropyLoss()\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    predict_label_full_train = []\n",
    "    predict_label_full = []\n",
    "    true_label_full_train = []\n",
    "    true_label_full = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        predict_label_train =[]\n",
    "        true_label_train =[]\n",
    "        running_loss_train = 0.0\n",
    "        for images, labels in tqdm(train_image, desc='Training loop'):\n",
    "            # Move inputs and labels to the device\n",
    "            images = images.to(torch.float)\n",
    "            image, label = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(image)\n",
    "            #print(outputs)\n",
    "            #print(label)\n",
    "            loss_train = criterion(outputs.float(), label.float())\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss_train.item() * label.size(0)\n",
    "            try:\n",
    "                _pred_train = outputs.cpu().data.numpy().astype(int).T[0].tolist()\n",
    "                if(len(_pred_train) == iter_size):\n",
    "                    predict_label_train.append(_pred_train)\n",
    "                    _true_train = label.cpu().data.numpy().astype(int).T.tolist()\n",
    "                    true_label_train.append(_true_train)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Concatenation error iter: {e}\")\n",
    "                print(_pred_train)\n",
    "                print(predict_label_train)    \n",
    "        train_loss = running_loss_train / len(train_image.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        try:\n",
    "            _p_train = predict_label_train\n",
    "            _t_train = true_label_train\n",
    "            predict_label_full_train.append(_p_train)\n",
    "            true_label_full_train.append(_t_train)\n",
    "        except Exception as e:\n",
    "            print(f\"Concatenation error full: {e}\")\n",
    "            print(predict_label_train)\n",
    "            print(true_label_train)\n",
    "        model.eval()\n",
    "        running_loss_valid = 0.0\n",
    "        rotulos =[] \n",
    "        predict_label =[]\n",
    "        true_label =[]\n",
    "        _iter=0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_image, desc='Validation loop'):\n",
    "                # Move inputs and labels to the device\n",
    "                images = images.to(torch.float)\n",
    "                images, label = images.to(device), labels.to(device)\n",
    "                rotulos.append(label.cpu().data.numpy())\n",
    "                outputs = model(images)\n",
    "                loss_valid = criterion(outputs.float(), label.float())\n",
    "                #print( [outputs.cpu().data.numpy().astype(int).T[0]])\n",
    "                #print(label.cpu().data.numpy().astype(int).T[0])\n",
    "                #print(predict_label)\n",
    "                try:\n",
    "                    _pred = outputs.cpu().data.numpy().astype(int).T[0].tolist()\n",
    "\n",
    "                    if(len(_pred) == iter_size):\n",
    "                        predict_label.append(_pred)\n",
    "                        _true = label.cpu().data.numpy().astype(int).T.tolist()\n",
    "                        true_label.append(_true)\n",
    "                except Exception as e:\n",
    "                    print(f\"Concatenation error iter: {e}\")\n",
    "                    print(_pred)\n",
    "                    print(predict_label)    \n",
    "                running_loss_valid += loss_valid.item() * label.size(0)\n",
    "                _iter +=1\n",
    "        val_loss = running_loss_valid / len(val_image.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f'End validation for epoch {epoch}')\n",
    "        print(f'Amount of images validated: {val_image}')\n",
    "        print(f'Label predict shape : {len(predict_label)} for epoch {epoch}')\n",
    "        print(f'Count of iterations: {_iter} for epoch {epoch}')\n",
    "        try:\n",
    "            _p = predict_label\n",
    "            _t = true_label\n",
    "            predict_label_full.append(_p)\n",
    "            true_label_full.append(_t)\n",
    "        except Exception as e:\n",
    "            print(f\"Concatenation error full: {e}\")\n",
    "            print(predict_label)\n",
    "            print(true_label)\n",
    "        #val_acc = accuracy_score(rotulos,output_model)\n",
    "        print(f'Val accuracy {epoch}:')\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss}, Validation loss: {val_loss}\")\n",
    "\n",
    "    predict_label_full_out = np.array(predict_label_full)\n",
    "    true_label_full_out = np.array(true_label_full)\n",
    "    predict_label_full_train_out = np.array(predict_label_full_train)\n",
    "    true_label_full_train_out = np.array(true_label_full_train)\n",
    "    print(f'Salvado das metricas de validacao e treino')\n",
    "    salvar_metricas(path=f'output/metricas/valid/fold_{fold_index}', name_file_train=f'predict_label_valid_fold_{fold_index}.npy', name_file_val=f'true_label_valid_fold_{fold_index}.npy', predict_label= predict_label_full_out, true_label= true_label_full_out)\n",
    "    salvar_metricas(path=f'output/metricas/train/fold_{fold_index}', name_file_train=f'predict_label_train_fold_{fold_index}.npy', name_file_val=f'true_label_train_fold_{fold_index}.npy', predict_label= predict_label_full_train_out, true_label= true_label_full_train_out)\n",
    "    print(f'Finalizado o salvamento das metricas')\n",
    "    return train_losses, val_losses, model,predict_label_full_out, true_label_full_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df8a989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "Train and valid for Fold 0\n",
      "Number of training images per iteration: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827dc7b2fc674c1392343a18be6c6ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48e9958c5274b0fbbaabc809e73bb40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 0\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7ee2bd02b150>\n",
      "Label predict shape : 21 for epoch 0\n",
      "Count of iterations: 22 for epoch 0\n",
      "Val accuracy 0:\n",
      "Epoch 1/2 - Train loss: 0.8509468400367984, Validation loss: 11961243.453703703\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0bcf0cd43b43cf997f85f041668086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92cb594b4ad34d6ea2157e025e7032bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 1\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7ee2bd02b150>\n",
      "Label predict shape : 21 for epoch 1\n",
      "Count of iterations: 22 for epoch 1\n",
      "Val accuracy 1:\n",
      "Epoch 2/2 - Train loss: 0.5782824064846392, Validation loss: 2432.469566062645\n",
      "Salvado das metricas de validacao e treino\n",
      "Pasta output/metricas/valid/fold_0 ja existe.\n",
      "Metricas salvas em output/metricas/valid/fold_0 com os nomes predict_label_valid_fold_0.npy e true_label_valid_fold_0.npy e tamanhos (2, 21, 5) e (2, 21, 5)\n",
      "Pasta output/metricas/train/fold_0 ja existe.\n",
      "Metricas salvas em output/metricas/train/fold_0 com os nomes predict_label_train_fold_0.npy e true_label_train_fold_0.npy e tamanhos (2, 21, 5) e (2, 21, 5)\n",
      "Finalizado o salvamento das metricas\n",
      "Fold 1:\n",
      "Train and valid for Fold 1\n",
      "Number of training images per iteration: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70c369fe7d84607b464ff7c33d926c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8651b73a0bc142ca95315d92390e2db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 0\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7ee2bd028d50>\n",
      "Label predict shape : 21 for epoch 0\n",
      "Count of iterations: 22 for epoch 0\n",
      "Val accuracy 0:\n",
      "Epoch 1/2 - Train loss: 0.6979328522251712, Validation loss: 28237861.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8961f7b72f54d628734a5e5f26bc22d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734106b60ade45719cab1b8ab854eb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 1\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7ee2bd028d50>\n",
      "Label predict shape : 21 for epoch 1\n",
      "Count of iterations: 22 for epoch 1\n",
      "Val accuracy 1:\n",
      "Epoch 2/2 - Train loss: 0.5218341262252243, Validation loss: 799.1220143636068\n",
      "Salvado das metricas de validacao e treino\n",
      "Pasta output/metricas/valid/fold_1 ja existe.\n",
      "Metricas salvas em output/metricas/valid/fold_1 com os nomes predict_label_valid_fold_1.npy e true_label_valid_fold_1.npy e tamanhos (2, 21, 5) e (2, 21, 5)\n",
      "Pasta output/metricas/train/fold_1 ja existe.\n",
      "Metricas salvas em output/metricas/train/fold_1 com os nomes predict_label_train_fold_1.npy e true_label_train_fold_1.npy e tamanhos (2, 21, 5) e (2, 21, 5)\n",
      "Finalizado o salvamento das metricas\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loss_total = []\n",
    "val_loss_total =[]\n",
    "all_models =[]\n",
    "epochs = 2\n",
    "## create first model.\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train_dataset)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    #print(f\"  Train: index={train_index}\")\n",
    "    #print(f\"  Test:  index={test_index}\")\n",
    "    ## init train test for folder\n",
    "    train_dataset_part = Subset( train_dataset, train_index)\n",
    "    val_dataset_part = Subset( train_dataset, test_index)\n",
    "\n",
    "    train_loader_img = DataLoader(train_dataset_part, batch_size=5, shuffle=True)\n",
    "    val_loader_img = DataLoader(val_dataset_part, batch_size=5, shuffle=True)\n",
    "\n",
    "    model= ECGClassifierResnet( num_classes=1)\n",
    "    if(False):\n",
    "        salvar_model(model, path='output/modelos', name_file=f'model_fold_{i}.pth')\n",
    "    print(f'Train and valid for Fold {i}')\n",
    "    t, l,_,outputs,labels = simple_loop(model, train_loader_img,val_loader_img, epochs, batch_size=5, fold_index=i)\n",
    "    ## Evaluate model.\n",
    "    train_loss_total.append(t)\n",
    "    val_loss_total.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc2494b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616]],\n",
       "\n",
       "       [[   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17]],\n",
       "\n",
       "       [[    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2]],\n",
       "\n",
       "       [[    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0]],\n",
       "\n",
       "       [[    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs\n",
    "## formato ddos outputs [epoch][batch][labels]\n",
    "#out = np.array(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38154859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0],\n",
       "       [1, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels[0][0:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345f4f10",
   "metadata": {},
   "source": [
    "## Métricas de Avaliacao dos modelos\n",
    "\n",
    "Acuracia\n",
    "\n",
    "Precisao\n",
    "\n",
    "Sensibilidade\n",
    "\n",
    "Especificidade\n",
    "\n",
    "F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a0dd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 0 1 1 0]\n",
      "  [1 0 0 1 0]\n",
      "  [0 0 1 1 1]\n",
      "  [0 0 0 0 0]\n",
      "  [1 1 1 1 0]\n",
      "  [1 1 0 0 0]\n",
      "  [1 0 1 1 1]\n",
      "  [0 0 1 0 1]\n",
      "  [0 0 0 0 0]\n",
      "  [1 0 1 1 0]\n",
      "  [0 0 1 1 1]\n",
      "  [0 0 0 1 0]\n",
      "  [1 0 0 1 1]\n",
      "  [1 1 1 1 0]\n",
      "  [0 1 0 1 0]\n",
      "  [1 0 0 1 0]\n",
      "  [1 1 1 1 0]\n",
      "  [1 1 0 1 1]\n",
      "  [1 1 1 1 1]\n",
      "  [0 0 0 1 1]\n",
      "  [0 0 1 1 1]]\n",
      "\n",
      " [[1 0 0 0 0]\n",
      "  [1 0 1 0 1]\n",
      "  [1 1 0 1 1]\n",
      "  [1 0 0 0 1]\n",
      "  [1 1 0 1 1]\n",
      "  [1 0 0 1 1]\n",
      "  [0 1 1 1 0]\n",
      "  [0 1 0 1 1]\n",
      "  [0 1 1 1 1]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 0 1]\n",
      "  [0 0 1 1 1]\n",
      "  [0 1 1 1 0]\n",
      "  [0 0 0 1 1]\n",
      "  [0 1 0 1 0]\n",
      "  [1 1 1 1 1]\n",
      "  [1 0 1 0 1]\n",
      "  [1 0 1 0 1]\n",
      "  [1 0 0 0 0]\n",
      "  [0 1 0 1 1]\n",
      "  [0 1 1 1 1]]]\n"
     ]
    }
   ],
   "source": [
    "#### Leitura de métricas salvas\n",
    "import numpy as np\n",
    "predict_label_0 = np.load('/Users/leonardocipriani/Documents/dev/python/Artificial Intelligence Projects/oca-ia/incor_env/output/metricas/train/fold_0/predict_label_train_fold_0.npy')\n",
    "true_label_0 = np.load('/Users/leonardocipriani/Documents/dev/python/Artificial Intelligence Projects/oca-ia/incor_env/output/metricas/train/fold_0/true_label_train_fold_0.npy')\n",
    "# --- IGNORE ---\n",
    "print(true_label_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1251295b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [4, 4, 4, 4, 4],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_label_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd73f641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_label_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9399a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2821440 2821440 2821440 2821440 2821440]\n",
      "  [3546063 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 4119212]\n",
      "  [2821440 2821440 3188174 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2471002]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [3436914 2821440 2821440 2821440 3913112]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 3827248 2821440 2821440]\n",
      "  [2821440 2821440 2631008 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2514506 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2847649 2821440 3869186 2821440]\n",
      "  [2759107 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 3113366]\n",
      "  [2821440 2821440 2821440 2821440 3379505]]\n",
      "\n",
      " [[    564     564    1204     564     564]\n",
      "  [    564     564     564     564     539]\n",
      "  [    564     564     564     564     564]\n",
      "  [    564     394     564     564     564]\n",
      "  [    564     564    1160     564     564]\n",
      "  [    564     564     564     564     564]\n",
      "  [    564     564     564     508     564]\n",
      "  [    564     264     564     564     564]\n",
      "  [    564     564     564     564    1093]\n",
      "  [    564     564    1578     564     564]\n",
      "  [    564     564     564     564     564]\n",
      "  [    564     564    2208     564     564]\n",
      "  [    564     564     564    1628     564]\n",
      "  [    564     564     564    1815     564]\n",
      "  [    564     564     564     564     564]\n",
      "  [    666     564     564     564     564]\n",
      "  [    564     564     564     564     564]\n",
      "  [    564     564     564     564     564]\n",
      "  [    564     564     564     564    2193]\n",
      "  [    564     564     564     564     564]\n",
      "  [    564     564     564     169     676]]]\n",
      "[[0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1]\n",
      " [1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#### Leitura de métricas salvas\n",
    "import numpy as np\n",
    "predict_label_0_valid= np.load('/Users/leonardocipriani/Documents/dev/python/Artificial Intelligence Projects/oca-ia/incor_env/output/metricas/valid/fold_0/predict_label_valid_fold_0.npy')\n",
    "true_label_0_valid= np.load('/Users/leonardocipriani/Documents/dev/python/Artificial Intelligence Projects/oca-ia/incor_env/output/metricas/valid/fold_0/true_label_valid_fold_0.npy')\n",
    "# --- IGNORE ---\n",
    "print(predict_label_0_valid)\n",
    "print(true_label_0_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc67921",
   "metadata": {},
   "source": [
    "## Criacao graficos de treinamento e validacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f25929",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Criar graficos de treinamento e validacao\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(5):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(train_loss_total[i], label='Train Loss')\n",
    "    plt.plot(val_loss_total[i], label='Validation Loss')\n",
    "    plt.title(f'Fold {i} - Train and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'output/graficos/loss_fold_{i}.png')\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fdd0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source incor_env/Scripts/activate\n"
     ]
    }
   ],
   "source": [
    "activate_script = os.path.join(\"incor_env\", \"Scripts\", \"activate\")\n",
    "if os.name == 'nt':  # Windows\n",
    "    activate_command = f\"{activate_script}\"\n",
    "else:  # macOS/Linux\n",
    "    activate_command = f\"source {activate_script}\"\n",
    "print(activate_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86485574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando ambiente virtual...\n",
      "total 1680\n",
      "drwxr-xr-x@ 19 leonardocipriani  staff     608 Dec 14 10:14 \u001b[34m.\u001b[m\u001b[m\n",
      "drwxr-xr-x@ 16 leonardocipriani  staff     512 Nov 25 13:15 \u001b[34m..\u001b[m\u001b[m\n",
      "drwxr-xr-x@ 16 leonardocipriani  staff     512 Dec 14 10:01 \u001b[34m.git\u001b[m\u001b[m\n",
      "-rw-r--r--   1 leonardocipriani  staff      13 Sep 13 17:55 .gitignore\n",
      "drwxr-xr-x   7 leonardocipriani  staff     224 Dec 14 10:14 \u001b[34m.incor_env\u001b[m\u001b[m\n",
      "drwxr-xr-x   4 leonardocipriani  staff     128 Dec 13 16:10 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\n",
      "drwxr-xr-x   9 leonardocipriani  staff     288 Oct 14 20:26 \u001b[34m.venv\u001b[m\u001b[m\n",
      "-rw-r--r--   1 leonardocipriani  staff    1967 Dec 11 07:20 README.md\n",
      "-rw-r--r--   1 leonardocipriani  staff  683671 Dec  7 10:06 classificador_ml.ipynb\n",
      "-rw-r--r--   1 leonardocipriani  staff   70005 Dec 14 10:15 classificador_oca copy.ipynb\n",
      "-rw-r--r--   1 leonardocipriani  staff   84135 Dec 13 16:18 classificador_oca.ipynb\n",
      "drwxr-xr-x   9 leonardocipriani  staff     288 Dec 13 16:18 \u001b[34mdataset\u001b[m\u001b[m\n",
      "-rw-r--r--@  1 leonardocipriani  staff     632 Sep  3 16:31 exemplo_csv.csv\n",
      "drwxr-xr-x   3 leonardocipriani  staff      96 Sep 28 18:46 \u001b[34mimgs\u001b[m\u001b[m\n",
      "drwxr-xr-x   9 leonardocipriani  staff     288 Dec 13 18:58 \u001b[34mincor_env\u001b[m\u001b[m\n",
      "drwxr-xr-x   4 leonardocipriani  staff     128 Dec  7 10:06 \u001b[34mlib\u001b[m\u001b[m\n",
      "drwxr-xr-x   3 leonardocipriani  staff      96 Sep  9 01:39 \u001b[34mmodels\u001b[m\u001b[m\n",
      "drwxr-xr-x   5 leonardocipriani  staff     160 Dec  7 10:06 \u001b[34moutput\u001b[m\u001b[m\n",
      "-rw-r--r--   1 leonardocipriani  staff     133 Dec  7 10:06 requirements.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def config_venv():\n",
    "    print(\"Criando ambiente virtual...\")\n",
    "    os.system(\"ls -la\")    \n",
    "    os.system(\"python -m venv .incor_env\")\n",
    "\n",
    "config_venv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870787b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ds_file = pd.read_csv('dataset/oca_incor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_file.rename(columns={ds_file.columns[0]:'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2e372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image1.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image2.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image3.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image4.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image5.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>image16.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>image16.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>image16.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>image16.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>image16.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           label  label\n",
       "0     image1.png      0\n",
       "1     image2.png      0\n",
       "2     image3.png      1\n",
       "3     image4.png      1\n",
       "4     image5.png      1\n",
       "..           ...    ...\n",
       "211  image16.png      1\n",
       "212  image16.png      0\n",
       "213  image16.png      0\n",
       "214  image16.png      1\n",
       "215  image16.png      1\n",
       "\n",
       "[216 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488fbc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

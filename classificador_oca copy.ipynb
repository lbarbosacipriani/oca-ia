{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb18702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (2.3.2)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (11.3.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (2.3.2)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 5)) (1.7.2)\n",
      "Requirement already satisfied: timm in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 6)) (1.0.20)\n",
      "Requirement already satisfied: jupyter_contrib_nbextensions in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 8)) (8.1.7)\n",
      "Requirement already satisfied: widgetsnbextension in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 9)) (4.0.14)\n",
      "Collecting pandas-profiling (from -r requirements.txt (line 10))\n",
      "  Using cached pandas_profiling-3.2.0-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 11)) (3.10.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (2025.9.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.13/site-packages (from timm->-r requirements.txt (line 6)) (0.23.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.13/site-packages (from timm->-r requirements.txt (line 6)) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.13/site-packages (from timm->-r requirements.txt (line 6)) (0.35.3)\n",
      "Requirement already satisfied: safetensors in ./.venv/lib/python3.13/site-packages (from timm->-r requirements.txt (line 6)) (0.6.2)\n",
      "Requirement already satisfied: ipython_genutils in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: jupyter_contrib_core>=0.3.3 in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (0.4.2)\n",
      "Requirement already satisfied: jupyter_core in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (5.8.1)\n",
      "Requirement already satisfied: jupyter_highlight_selected_word>=0.1.1 in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: jupyter_nbextensions_configurator>=0.4.0 in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (0.6.4)\n",
      "Requirement already satisfied: nbconvert>=6.0 in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (7.16.6)\n",
      "Requirement already satisfied: notebook>=6.0 in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (7.4.7)\n",
      "Requirement already satisfied: tornado in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=4.1 in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (5.14.3)\n",
      "Requirement already satisfied: lxml in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (6.0.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.13/site-packages (from ipywidgets->-r requirements.txt (line 8)) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.13/site-packages (from ipywidgets->-r requirements.txt (line 8)) (9.5.0)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./.venv/lib/python3.13/site-packages (from ipywidgets->-r requirements.txt (line 8)) (3.0.15)\n",
      "INFO: pip is looking at multiple versions of pandas-profiling to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached pandas_profiling-3.1.0-py2.py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached pandas_profiling-3.0.0-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pydantic>=1.8.1 (from pandas-profiling->-r requirements.txt (line 10))\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting visions==0.7.1 (from visions[type_image_path]==0.7.1->pandas-profiling->-r requirements.txt (line 10))\n",
      "  Using cached visions-0.7.1-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting htmlmin>=0.1.12 (from pandas-profiling->-r requirements.txt (line 10))\n",
      "  Using cached htmlmin-0.1.12.tar.gz (19 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[27 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/Users/leonardocipriani/Documents/dev/python/Artificial Intelligence Projects/oca-ia/.venv/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/Users/leonardocipriani/Documents/dev/python/Artificial Intelligence Projects/oca-ia/.venv/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m                              \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/Users/leonardocipriani/Documents/dev/python/Artificial Intelligence Projects/oca-ia/.venv/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/30/pgp77_r92w5_rn1ghx4kn51h0000gn/T/pip-build-env-cqbyznbx/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/30/pgp77_r92w5_rn1ghx4kn51h0000gn/T/pip-build-env-cqbyznbx/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m301\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/30/pgp77_r92w5_rn1ghx4kn51h0000gn/T/pip-build-env-cqbyznbx/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/30/pgp77_r92w5_rn1ghx4kn51h0000gn/T/pip-build-env-cqbyznbx/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m4\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/30/pgp77_r92w5_rn1ghx4kn51h0000gn/T/pip-install-trpv51sk/htmlmin_853f4fd3cb804ae4bd8735757845629e/htmlmin/__init__.py\"\u001b[0m, line \u001b[35m28\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     from .main import minify, Minifier\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/30/pgp77_r92w5_rn1ghx4kn51h0000gn/T/pip-install-trpv51sk/htmlmin_853f4fd3cb804ae4bd8735757845629e/htmlmin/main.py\"\u001b[0m, line \u001b[35m28\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     import cgi\n",
      "  \u001b[31m   \u001b[0m \u001b[1;35mModuleNotFoundError\u001b[0m: \u001b[35mNo module named 'cgi'\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40d20cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image, ImageChops\n",
    "import numpy as np\n",
    "import torch\n",
    "from lib.ImageFIlter import treat_image_PIL\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import  DataLoader, TensorDataset, Dataset\n",
    "from torch import nn\n",
    "import timm\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22170478",
   "metadata": {},
   "source": [
    "## Exemplo de classificador com leitura do CSV. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de731cfa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9688a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Cria funcao para validar se pasta a ser inserida existe. Caso nao exista, cria a pasta\n",
    "def create_folder_if_not_exists(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f'Pasta {folder_path} criada.')\n",
    "    else:\n",
    "        print(f'Pasta {folder_path} ja existe.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a724514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta output ja existe.\n",
      "Pasta models ja existe.\n"
     ]
    }
   ],
   "source": [
    "## Criacao de pastas output e models se nao existirem\n",
    "import os\n",
    "create_folder_if_not_exists('output')\n",
    "create_folder_if_not_exists('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd08e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_file = pd.read_csv('dataset/oca.csv')\n",
    "ds_file = ds_file[['path','oca']]\n",
    "ds_file.rename(columns={'oca':'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10c59064",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_file.to_csv('dataset/oca_incor.csv', index=False)\n",
    "ds_file = pd.read_csv('dataset/oca_incor.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52323b9c",
   "metadata": {},
   "source": [
    "## Geracao Tensor com imagens filtradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9eaf296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop\n",
    "img_dataset = np.ones((ds_file.shape[0],3,256,256),dtype=np.uint8)\n",
    "\n",
    "j=0\n",
    "for i in ds_file['path']:\n",
    "    img_dataset[j]=treat_image_PIL('dataset/path/'+i,2)\n",
    "    j+=1\n",
    "tensor_imagem = torch.tensor(img_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966dbdca",
   "metadata": {},
   "source": [
    "## Geracao de rótulos para classificação\n",
    "\n",
    "Colunas utilizadas para classificacao (Labels)\n",
    "\n",
    "labels = [ OCA, NOCA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29f9adfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_label = torch.tensor(np.array(ds_file['label'].astype(int)))\n",
    "tensor_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bd06990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "ds_file.columns\n",
    "labels = [ 'label']\n",
    "ds_labels = ds_file[labels]\n",
    "ds_labels=ds_labels.astype(int)#,'False':0})\n",
    "tensor_label = torch.tensor(np.array(ds_file['label'].astype(int)))\n",
    "tensor_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb9fcbf",
   "metadata": {},
   "source": [
    "### Resumo:\n",
    "\n",
    "model(tensor_imagem,ds_id_sex_et)\n",
    "\n",
    "labels : tensor_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc188221",
   "metadata": {},
   "source": [
    "## Criacao Subset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fdd31e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead13f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f0dcb69",
   "metadata": {},
   "source": [
    "## K-fold = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38e96965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=2, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=2)\n",
    "kf.get_n_splits(tensor_imagem)\n",
    "print(kf)\n",
    "#for i in enumerate(kf.split(tensor_imagem)):\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64652bf",
   "metadata": {},
   "source": [
    "## Criacao do dataset de treino e validacao com batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "925d5185",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "train_dataset = TensorDataset(tensor_imagem, tensor_label)\n",
    "train_loader_img = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d682980b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba835adf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "addfb539",
   "metadata": {},
   "source": [
    "## Create a model to concat layers\n",
    "\n",
    "O modelo utilizado será o mesmo do artigo: Artificial Intelligence-Driven Screening System for Rapid Image-Based Classification of 12-Lead ECG Exams: A Promising Solution for Emergency Room Prioritization \n",
    "\n",
    "Desenvolvido conforme a figura abaixo: \n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f32068ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models import ECGClassifierResnet\n",
    "\n",
    "class ECGClassifierResnet(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(ECGClassifierResnet, self).__init__()\n",
    "        # Where we define all the parts of the model\n",
    "        #self.base_model = timm.create_model('efficientnet_b0', pretrained=True) \n",
    "        self.base_model=timm.create_model('resnet50d.ra4_e3600_r224_in1k',pretrained=True)\n",
    "        #self.base_model = timm.create_model('vit_mediumd_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k',num_classes=5,pretrained=True)\n",
    "\n",
    "\n",
    "        self.features = nn.Sequential(*list(self.base_model.children())[:-1])\n",
    "\n",
    "        enet_out_size = 2048        # Make a classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(enet_out_size,1)\n",
    "        ) # saida como Sigmoid multilabel\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Connect these parts and return the output\n",
    "        x = self.features(x)\n",
    "        output = self.classifier(x)\n",
    "        #output = nn.Softmax(dim=1)(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c98c40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c198fcb9",
   "metadata": {},
   "source": [
    "## Train - Valid Loop\n",
    "\n",
    "Fluxo de treinamento\n",
    "\n",
    "Input: modelo, dataloader_train, dataloader_teste , epocas.\n",
    "- Para cada epoca\n",
    "\n",
    "    - treinamento:\n",
    "\n",
    "        - para cada batch (lote de imagens) no dataloader_train: (executo para todo o dataloader)\n",
    "            \n",
    "            leitura das imagens e rotulos. \n",
    "            \n",
    "            prediz o rotulo (outputs): rotulos preditos para o tamanho do batch (ex: para um batch de 10, o \n",
    "            output tem tamanho 10)\n",
    "            \n",
    "            calcula o Loss () \n",
    "            \n",
    "            aplica o backward -> Ajuste dos pesos nos neuronios de acordo com o valor de loss (w_{i})\n",
    "\n",
    "            otimizo o modelo com optimizer.step()\n",
    "\n",
    "    - validacao (avaliacao do desempenho para aquele conjunto de treinamento):\n",
    "        - para cada batch (lote de imagens) no dataloader_train: (executo para todo o dataloader)\n",
    "\n",
    "            leitura das imagens e rotulos.\n",
    "\n",
    "            prediz o rotulo (outputs): rotulos preditos para o tamanho do batch (ex: para um batch de 10, o \n",
    "            output tem tamanho 10)\n",
    "            \n",
    "            calcula o Loss ()\n",
    "\n",
    "\n",
    "    - Calculo de metricas de loss media de validacao.\n",
    "\n",
    "    - Calculo de metrica de loss media de treinamento. \n",
    "\n",
    "    - Print de metricas de desempenho para a epoca. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "809e6d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_model(model, path = 'output/modelos', name_file='model.pth'):\n",
    "    full_path = os.path.join(path, name_file)\n",
    "    torch.save(model.state_dict(), full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60622c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_metricas(path, name_file_train='train_loss_total.npy', name_file_val='val_loss_total.npy',predict_label=None, true_label=None):\n",
    "    create_folder_if_not_exists(path)\n",
    "    full_path_train = os.path.join(path, name_file_train)\n",
    "    full_path_val = os.path.join(path, name_file_val)\n",
    "    np.save(full_path_train, np.array(predict_label))\n",
    "    np.save(full_path_val, np.array(true_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5d5e01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \" + str(device))\n",
    "def simple_loop(model, train_image, val_image, epochs):\n",
    "    # Simple training loop\n",
    "    num_epochs = epochs\n",
    "    train_losses, val_losses = [], []\n",
    "    lim_loss = 1.5\n",
    "    iter_size = 5\n",
    "    print(f'Number of training images per iteration: {iter_size}')\n",
    "    #model = modelo( num_classes=5)\n",
    "    model.to(device)\n",
    "    criterion =  nn.L1Loss()\n",
    "    #criterion =  nn.CrossEntropyLoss()\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    predict_label_full_train = []\n",
    "    predict_label_full = []\n",
    "    true_label_full_train = []\n",
    "    true_label_full = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        predict_label_train =[]\n",
    "        true_label_train =[]\n",
    "        running_loss_train = 0.0\n",
    "        for images, labels in tqdm(train_image, desc='Training loop'):\n",
    "            # Move inputs and labels to the device\n",
    "            images = images.to(torch.float)\n",
    "            image, label = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(image)\n",
    "            #print(outputs)\n",
    "            #print(label)\n",
    "            loss_train = criterion(outputs.float(), label.float())\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss_train.item() * label.size(0)\n",
    "            try:\n",
    "                _pred_train = outputs.cpu().data.numpy().astype(int).T[0].tolist()\n",
    "                if(len(_pred_train) == iter_size):\n",
    "                    predict_label_train.append(_pred_train)\n",
    "                    _true_train = label.cpu().data.numpy().astype(int).T[0].tolist()\n",
    "                    true_label_train.append(_true_train)\n",
    "            except Exception as e:\n",
    "                print(f\"Concatenation error iter: {e}\")\n",
    "                print(_pred_train)\n",
    "                print(predict_label_train)    \n",
    "        train_loss = running_loss_train / len(train_image.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        try:\n",
    "            _p_train = predict_label_train\n",
    "            _t_train = _true_train\n",
    "            predict_label_full_train.append(_p_train)\n",
    "            true_label_full_train.append(_t_train)\n",
    "        except Exception as e:\n",
    "            print(f\"Concatenation error full: {e}\")\n",
    "            print(predict_label_train)\n",
    "            print(true_label_train)\n",
    "        model.eval()\n",
    "        running_loss_valid = 0.0\n",
    "        rotulos =[] \n",
    "        predict_label =[]\n",
    "        true_label =[]\n",
    "        _iter=0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_image, desc='Validation loop'):\n",
    "                # Move inputs and labels to the device\n",
    "                images = images.to(torch.float)\n",
    "                images, label = images.to(device), labels.to(device)\n",
    "                rotulos.append(label.cpu().data.numpy())\n",
    "                outputs = model(images)\n",
    "                loss_valid = criterion(outputs.float(), label.float())\n",
    "                #print( [outputs.cpu().data.numpy().astype(int).T[0]])\n",
    "                #print(label.cpu().data.numpy().astype(int).T[0])\n",
    "                #print(predict_label)\n",
    "                try:\n",
    "                    _pred = outputs.cpu().data.numpy().astype(int).T[0].tolist()\n",
    "                    if(len(_pred) == iter_size):\n",
    "                        predict_label.append(_pred)\n",
    "                        _true = label.cpu().data.numpy().astype(int).T[0].tolist()\n",
    "                        true_label.append(_true)\n",
    "                except Exception as e:\n",
    "                    print(f\"Concatenation error iter: {e}\")\n",
    "                    print(_pred)\n",
    "                    print(predict_label)    \n",
    "                running_loss_valid += loss_valid.item() * label.size(0)\n",
    "                _iter +=1\n",
    "        val_loss = running_loss_valid / len(val_image.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f'End validation for epoch {epoch}')\n",
    "        print(f'Amount of images validated: {val_image}')\n",
    "        print(f'Label predict shape : {len(predict_label)} for epoch {epoch}')\n",
    "        print(f'Count of iterations: {_iter} for epoch {epoch}')\n",
    "        try:\n",
    "            _p = predict_label\n",
    "            _t = true_label\n",
    "            predict_label_full.append(_p)\n",
    "            true_label_full.append(_t)\n",
    "        except Exception as e:\n",
    "            print(f\"Concatenation error full: {e}\")\n",
    "            print(predict_label)\n",
    "            print(true_label)\n",
    "        #val_acc = accuracy_score(rotulos,output_model)\n",
    "        print(f'Val accuracy {epoch}:')\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss}, Validation loss: {val_loss}\")\n",
    "\n",
    "    predict_label_full_out = np.array(predict_label_full)\n",
    "    true_label_full_out = np.array(true_label_full)\n",
    "    predict_label_full_train_out = np.array(predict_label_full_train)\n",
    "    true_label_full_train_out = np.array(true_label_full_train)\n",
    "    print(f'Salvado das metricas de validacao e treino')\n",
    "    salvar_metricas(path='output/metricas/valid', name_file_train=f'predict_label_valid{i}.npy', name_file_val=f'true_label_valid{i}.npy', predict_label= predict_label_full_out, true_label= true_label_full_out)\n",
    "    salvar_metricas(path='output/metricas/train', name_file_train=f'predict_label_train{i}.npy', name_file_val=f'true_label_train{i}.npy', predict_label= predict_label_full_train_out, true_label= true_label_full_train_out)\n",
    "    print(f'Finalizado o salvamento das metricas')\n",
    "    return train_losses, val_losses, model,predict_label_full_out, true_label_full_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4df8a989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "Train and valid for Fold 0\n",
      "Number of training images per iteration: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b573d83c522249c5a9e9ddb5c2a1c8ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb1b25826e645a5bad1ed502dcd5ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 0\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x149721850>\n",
      "Label predict shape : 21 for epoch 0\n",
      "Count of iterations: 22 for epoch 0\n",
      "Val accuracy 0:\n",
      "Epoch 1/2 - Train loss: 0.7354033765969453, Validation loss: 87841.57262731482\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a397412708a47b09835308d40570167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cdff46f9f0487bb70364edec55e15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 1\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x149721850>\n",
      "Label predict shape : 21 for epoch 1\n",
      "Count of iterations: 22 for epoch 1\n",
      "Val accuracy 1:\n",
      "Epoch 2/2 - Train loss: 0.5677434559221621, Validation loss: 264.4285871717665\n",
      "Salvado das metricas de validacao e treino\n",
      "Pasta output/metricas/valid criada.\n",
      "Pasta output/metricas/train criada.\n",
      "Finalizado o salvamento das metricas\n",
      "Fold 1:\n",
      "Train and valid for Fold 1\n",
      "Number of training images per iteration: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e9d425b9184b29a52ee9c73e581bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8e0637bbdc4d23946410f5b202c9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 0\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x149722050>\n",
      "Label predict shape : 21 for epoch 0\n",
      "Count of iterations: 22 for epoch 0\n",
      "Val accuracy 0:\n",
      "Epoch 1/2 - Train loss: 0.913980185157723, Validation loss: 99411.52835648147\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb06f4cb6a444967af0b2ba7ec9e753d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fd428fc59b4f4b87d7955197b03cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 1\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x149722050>\n",
      "Label predict shape : 21 for epoch 1\n",
      "Count of iterations: 22 for epoch 1\n",
      "Val accuracy 1:\n",
      "Epoch 2/2 - Train loss: 0.6170992837459953, Validation loss: 0.7603154706734198\n",
      "Salvado das metricas de validacao e treino\n",
      "Pasta output/metricas/valid ja existe.\n",
      "Pasta output/metricas/train ja existe.\n",
      "Finalizado o salvamento das metricas\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loss_total = []\n",
    "val_loss_total =[]\n",
    "all_models =[]\n",
    "epochs = 2\n",
    "## create first model.\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train_dataset)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    #print(f\"  Train: index={train_index}\")\n",
    "    #print(f\"  Test:  index={test_index}\")\n",
    "    ## init train test for folder\n",
    "    train_dataset_part = Subset( train_dataset, train_index)\n",
    "    val_dataset_part = Subset( train_dataset, test_index)\n",
    "\n",
    "    train_loader_img = DataLoader(train_dataset_part, batch_size=5, shuffle=True)\n",
    "    val_loader_img = DataLoader(val_dataset_part, batch_size=5, shuffle=True)\n",
    "\n",
    "    model= ECGClassifierResnet( num_classes=1)\n",
    "    salvar_model(model, path='output/modelos', name_file=f'model_fold_{i}.pth')\n",
    "    print(f'Train and valid for Fold {i}')\n",
    "    t, l,_,outputs,labels = simple_loop(model, train_loader_img,val_loader_img, epochs)\n",
    "    ## Evaluate model.\n",
    "    train_loss_total.append(t)\n",
    "    val_loss_total.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc2494b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616]],\n",
       "\n",
       "       [[   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17]],\n",
       "\n",
       "       [[    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2]],\n",
       "\n",
       "       [[    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0]],\n",
       "\n",
       "       [[    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs\n",
    "## formato ddos outputs [epoch][batch][labels]\n",
    "#out = np.array(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38154859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0],\n",
       "       [1, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels[0][0:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345f4f10",
   "metadata": {},
   "source": [
    "## Métricas de Avaliacao dos modelos\n",
    "\n",
    "Acuracia\n",
    "\n",
    "Precisao\n",
    "\n",
    "Sensibilidade\n",
    "\n",
    "Especificidade\n",
    "\n",
    "F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "70a0dd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 0 1 1 0]\n",
      "  [1 0 0 1 0]\n",
      "  [0 0 1 1 1]\n",
      "  [0 0 0 0 0]\n",
      "  [1 1 1 1 0]\n",
      "  [1 1 0 0 0]\n",
      "  [1 0 1 1 1]\n",
      "  [0 0 1 0 1]\n",
      "  [0 0 0 0 0]\n",
      "  [1 0 1 1 0]\n",
      "  [0 0 1 1 1]\n",
      "  [0 0 0 1 0]\n",
      "  [1 0 0 1 1]\n",
      "  [1 1 1 1 0]\n",
      "  [0 1 0 1 0]\n",
      "  [1 0 0 1 0]\n",
      "  [1 1 1 1 0]\n",
      "  [1 1 0 1 1]\n",
      "  [1 1 1 1 1]\n",
      "  [0 0 0 1 1]\n",
      "  [0 0 1 1 1]]\n",
      "\n",
      " [[1 0 0 0 0]\n",
      "  [1 0 1 0 1]\n",
      "  [1 1 0 1 1]\n",
      "  [1 0 0 0 1]\n",
      "  [1 1 0 1 1]\n",
      "  [1 0 0 1 1]\n",
      "  [0 1 1 1 0]\n",
      "  [0 1 0 1 1]\n",
      "  [0 1 1 1 1]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 0 1]\n",
      "  [0 0 1 1 1]\n",
      "  [0 1 1 1 0]\n",
      "  [0 0 0 1 1]\n",
      "  [0 1 0 1 0]\n",
      "  [1 1 1 1 1]\n",
      "  [1 0 1 0 1]\n",
      "  [1 0 1 0 1]\n",
      "  [1 0 0 0 0]\n",
      "  [0 1 0 1 1]\n",
      "  [0 1 1 1 1]]]\n"
     ]
    }
   ],
   "source": [
    "#### Leitura de métricas salvas\n",
    "import numpy as np\n",
    "predict_label_0 = np.load('/Users/leonardocipriani/Documents/dev/python/Artificial Intelligence Projects/oca-ia/incor_env/output/metricas/train/fold_0/predict_label_train_fold_0.npy')\n",
    "true_label_0 = np.load('/Users/leonardocipriani/Documents/dev/python/Artificial Intelligence Projects/oca-ia/incor_env/output/metricas/train/fold_0/true_label_train_fold_0.npy')\n",
    "# --- IGNORE ---\n",
    "print(true_label_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1251295b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [4, 4, 4, 4, 4],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_label_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd73f641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_label_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed9399a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2821440 2821440 2821440 2821440 2821440]\n",
      "  [3546063 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 4119212]\n",
      "  [2821440 2821440 3188174 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2471002]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [3436914 2821440 2821440 2821440 3913112]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 3827248 2821440 2821440]\n",
      "  [2821440 2821440 2631008 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2514506 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2847649 2821440 3869186 2821440]\n",
      "  [2759107 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 3113366]\n",
      "  [2821440 2821440 2821440 2821440 3379505]]\n",
      "\n",
      " [[    564     564    1204     564     564]\n",
      "  [    564     564     564     564     539]\n",
      "  [    564     564     564     564     564]\n",
      "  [    564     394     564     564     564]\n",
      "  [    564     564    1160     564     564]\n",
      "  [    564     564     564     564     564]\n",
      "  [    564     564     564     508     564]\n",
      "  [    564     264     564     564     564]\n",
      "  [    564     564     564     564    1093]\n",
      "  [    564     564    1578     564     564]\n",
      "  [    564     564     564     564     564]\n",
      "  [    564     564    2208     564     564]\n",
      "  [    564     564     564    1628     564]\n",
      "  [    564     564     564    1815     564]\n",
      "  [    564     564     564     564     564]\n",
      "  [    666     564     564     564     564]\n",
      "  [    564     564     564     564     564]\n",
      "  [    564     564     564     564     564]\n",
      "  [    564     564     564     564    2193]\n",
      "  [    564     564     564     564     564]\n",
      "  [    564     564     564     169     676]]]\n",
      "[[0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1]\n",
      " [1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#### Leitura de métricas salvas\n",
    "import numpy as np\n",
    "predict_label_0_valid= np.load('/Users/leonardocipriani/Documents/dev/python/Artificial Intelligence Projects/oca-ia/incor_env/output/metricas/valid/fold_0/predict_label_valid_fold_0.npy')\n",
    "true_label_0_valid= np.load('/Users/leonardocipriani/Documents/dev/python/Artificial Intelligence Projects/oca-ia/incor_env/output/metricas/valid/fold_0/true_label_valid_fold_0.npy')\n",
    "# --- IGNORE ---\n",
    "print(predict_label_0_valid)\n",
    "print(true_label_0_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc67921",
   "metadata": {},
   "source": [
    "## Criacao graficos de treinamento e validacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f25929",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Criar graficos de treinamento e validacao\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(5):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(train_loss_total[i], label='Train Loss')\n",
    "    plt.plot(val_loss_total[i], label='Validation Loss')\n",
    "    plt.title(f'Fold {i} - Train and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'output/graficos/loss_fold_{i}.png')\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6fdd0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source incor_env/Scripts/activate\n"
     ]
    }
   ],
   "source": [
    "activate_script = os.path.join(\"incor_env\", \"Scripts\", \"activate\")\n",
    "if os.name == 'nt':  # Windows\n",
    "    activate_command = f\"{activate_script}\"\n",
    "else:  # macOS/Linux\n",
    "    activate_command = f\"source {activate_script}\"\n",
    "print(activate_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86485574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando ambiente virtual...\n",
      "total 1680\n",
      "drwxr-xr-x@ 19 leonardocipriani  staff     608 Dec 14 10:14 \u001b[34m.\u001b[m\u001b[m\n",
      "drwxr-xr-x@ 16 leonardocipriani  staff     512 Nov 25 13:15 \u001b[34m..\u001b[m\u001b[m\n",
      "drwxr-xr-x@ 16 leonardocipriani  staff     512 Dec 14 10:01 \u001b[34m.git\u001b[m\u001b[m\n",
      "-rw-r--r--   1 leonardocipriani  staff      13 Sep 13 17:55 .gitignore\n",
      "drwxr-xr-x   7 leonardocipriani  staff     224 Dec 14 10:14 \u001b[34m.incor_env\u001b[m\u001b[m\n",
      "drwxr-xr-x   4 leonardocipriani  staff     128 Dec 13 16:10 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\n",
      "drwxr-xr-x   9 leonardocipriani  staff     288 Oct 14 20:26 \u001b[34m.venv\u001b[m\u001b[m\n",
      "-rw-r--r--   1 leonardocipriani  staff    1967 Dec 11 07:20 README.md\n",
      "-rw-r--r--   1 leonardocipriani  staff  683671 Dec  7 10:06 classificador_ml.ipynb\n",
      "-rw-r--r--   1 leonardocipriani  staff   70005 Dec 14 10:15 classificador_oca copy.ipynb\n",
      "-rw-r--r--   1 leonardocipriani  staff   84135 Dec 13 16:18 classificador_oca.ipynb\n",
      "drwxr-xr-x   9 leonardocipriani  staff     288 Dec 13 16:18 \u001b[34mdataset\u001b[m\u001b[m\n",
      "-rw-r--r--@  1 leonardocipriani  staff     632 Sep  3 16:31 exemplo_csv.csv\n",
      "drwxr-xr-x   3 leonardocipriani  staff      96 Sep 28 18:46 \u001b[34mimgs\u001b[m\u001b[m\n",
      "drwxr-xr-x   9 leonardocipriani  staff     288 Dec 13 18:58 \u001b[34mincor_env\u001b[m\u001b[m\n",
      "drwxr-xr-x   4 leonardocipriani  staff     128 Dec  7 10:06 \u001b[34mlib\u001b[m\u001b[m\n",
      "drwxr-xr-x   3 leonardocipriani  staff      96 Sep  9 01:39 \u001b[34mmodels\u001b[m\u001b[m\n",
      "drwxr-xr-x   5 leonardocipriani  staff     160 Dec  7 10:06 \u001b[34moutput\u001b[m\u001b[m\n",
      "-rw-r--r--   1 leonardocipriani  staff     133 Dec  7 10:06 requirements.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def config_venv():\n",
    "    print(\"Criando ambiente virtual...\")\n",
    "    os.system(\"ls -la\")    \n",
    "    os.system(\"python -m venv .incor_env\")\n",
    "\n",
    "config_venv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870787b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beb18702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (2.3.3)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (11.3.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (2.3.3)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 5)) (1.7.2)\n",
      "Requirement already satisfied: timm in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 6)) (1.0.20)\n",
      "Requirement already satisfied: jupyter_contrib_nbextensions in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 8)) (8.1.7)\n",
      "Requirement already satisfied: widgetsnbextension in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 9)) (4.0.14)\n",
      "Collecting pandas-profiling (from -r requirements.txt (line 10))\n",
      "  Using cached pandas_profiling-3.2.0-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 11)) (3.10.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.13/site-packages (from timm->-r requirements.txt (line 6)) (0.23.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.13/site-packages (from timm->-r requirements.txt (line 6)) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.13/site-packages (from timm->-r requirements.txt (line 6)) (0.35.3)\n",
      "Requirement already satisfied: safetensors in ./.venv/lib/python3.13/site-packages (from timm->-r requirements.txt (line 6)) (0.6.2)\n",
      "Requirement already satisfied: ipython_genutils in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: jupyter_contrib_core>=0.3.3 in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (0.4.2)\n",
      "Requirement already satisfied: jupyter_core in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (5.8.1)\n",
      "Requirement already satisfied: jupyter_highlight_selected_word>=0.1.1 in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: jupyter_nbextensions_configurator>=0.4.0 in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (0.6.4)\n",
      "Requirement already satisfied: nbconvert>=6.0 in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (7.16.6)\n",
      "Requirement already satisfied: notebook>=6.0 in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (7.4.7)\n",
      "Requirement already satisfied: tornado in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=4.1 in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (5.14.3)\n",
      "Requirement already satisfied: lxml in ./.venv/lib/python3.13/site-packages (from jupyter_contrib_nbextensions->-r requirements.txt (line 7)) (6.0.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.13/site-packages (from ipywidgets->-r requirements.txt (line 8)) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.13/site-packages (from ipywidgets->-r requirements.txt (line 8)) (9.6.0)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./.venv/lib/python3.13/site-packages (from ipywidgets->-r requirements.txt (line 8)) (3.0.15)\n",
      "INFO: pip is looking at multiple versions of pandas-profiling to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached pandas_profiling-3.1.0-py2.py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached pandas_profiling-3.0.0-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pydantic>=1.8.1 (from pandas-profiling->-r requirements.txt (line 10))\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting visions==0.7.1 (from visions[type_image_path]==0.7.1->pandas-profiling->-r requirements.txt (line 10))\n",
      "  Using cached visions-0.7.1-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting htmlmin>=0.1.12 (from pandas-profiling->-r requirements.txt (line 10))\n",
      "  Using cached htmlmin-0.1.12.tar.gz (19 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[27 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/home/leo/Documents/dev/oca-ia/.venv/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/home/leo/Documents/dev/oca-ia/.venv/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m                              \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/home/leo/Documents/dev/oca-ia/.venv/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-z_cpk24w/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-z_cpk24w/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m301\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-z_cpk24w/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-z_cpk24w/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m4\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-install-b7t6b7a7/htmlmin_5d7ad7ca284646349895524934c8bf0b/htmlmin/__init__.py\"\u001b[0m, line \u001b[35m28\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     from .main import minify, Minifier\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-install-b7t6b7a7/htmlmin_5d7ad7ca284646349895524934c8bf0b/htmlmin/main.py\"\u001b[0m, line \u001b[35m28\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     import cgi\n",
      "  \u001b[31m   \u001b[0m \u001b[1;35mModuleNotFoundError\u001b[0m: \u001b[35mNo module named 'cgi'\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b40d20cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image, ImageChops\n",
    "import numpy as np\n",
    "import torch\n",
    "from lib.ImageFIlter import treat_image_PIL\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import  DataLoader, TensorDataset, Dataset\n",
    "from torch import nn\n",
    "import timm\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22170478",
   "metadata": {},
   "source": [
    "## Exemplo de classificador com leitura do CSV. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de731cfa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9688a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Criacao de pastas output e models se nao existirem\n",
    "import os\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd08e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_file = pd.read_csv('dataset/oca.csv')\n",
    "ds_file = ds_file[['path','oca']]\n",
    "ds_file.rename(columns={'oca':'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10c59064",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_file.to_csv('dataset/oca_incor.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52323b9c",
   "metadata": {},
   "source": [
    "## Geracao Tensor com imagens filtradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eaf296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop\n",
    "img_dataset = np.ones((ds_file.shape[0],3,256,256),dtype=np.uint8)\n",
    "\n",
    "j=0\n",
    "for i in ds_file['path']:\n",
    "    img_dataset[j]=treat_image_PIL('dataset/path/'+i,2)\n",
    "    j+=1\n",
    "tensor_imagem = torch.tensor(img_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966dbdca",
   "metadata": {},
   "source": [
    "## Geracao de rótulos para classificação\n",
    "\n",
    "Colunas utilizadas para classificacao (Labels)\n",
    "\n",
    "labels = [ OCA, NOCA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bd06990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "ds_file.columns\n",
    "labels = [ 'label']\n",
    "ds_labels = ds_file[labels]\n",
    "ds_labels=ds_labels.astype(int)#,'False':0})\n",
    "tensor_label = torch.tensor(np.array(ds_labels))\n",
    "tensor_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb9fcbf",
   "metadata": {},
   "source": [
    "### Resumo:\n",
    "\n",
    "model(tensor_imagem,ds_id_sex_et)\n",
    "\n",
    "labels : tensor_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc188221",
   "metadata": {},
   "source": [
    "## Criacao Subset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26fdd31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subset(Dataset):\n",
    "    r\"\"\"\n",
    "    Subset of a dataset at specified indices.\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): The whole Dataset\n",
    "        indices (sequence): Indices in the whole set selected for subset\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    @property\n",
    "    def classes(self):\n",
    "        return self.data.classes\n",
    "\n",
    "    def shape(self):\n",
    "        return self.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead13f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f0dcb69",
   "metadata": {},
   "source": [
    "## K-fold = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38e96965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(tensor_imagem)\n",
    "print(kf)\n",
    "#for i in enumerate(kf.split(tensor_imagem)):\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64652bf",
   "metadata": {},
   "source": [
    "## Criacao do dataset de treino e validacao com batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "925d5185",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "train_dataset = TensorDataset(tensor_imagem, tensor_label)\n",
    "train_loader_img = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d682980b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba835adf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "addfb539",
   "metadata": {},
   "source": [
    "## Create a model to concat layers\n",
    "\n",
    "O modelo utilizado será o mesmo do artigo: Artificial Intelligence-Driven Screening System for Rapid Image-Based Classification of 12-Lead ECG Exams: A Promising Solution for Emergency Room Prioritization \n",
    "\n",
    "Desenvolvido conforme a figura abaixo: \n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f32068ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models import ECGClassifierResnet\n",
    "\n",
    "class ECGClassifierResnet(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(ECGClassifierResnet, self).__init__()\n",
    "        # Where we define all the parts of the model\n",
    "        #self.base_model = timm.create_model('efficientnet_b0', pretrained=True) \n",
    "        self.base_model=timm.create_model('resnet50d.ra4_e3600_r224_in1k',pretrained=True)\n",
    "        #self.base_model = timm.create_model('vit_mediumd_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k',num_classes=5,pretrained=True)\n",
    "\n",
    "\n",
    "        self.features = nn.Sequential(*list(self.base_model.children())[:-1])\n",
    "\n",
    "        enet_out_size = 2048        # Make a classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(enet_out_size,1)\n",
    "        ) # saida como Sigmoid multilabel\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Connect these parts and return the output\n",
    "        x = self.features(x)\n",
    "        output = self.classifier(x)\n",
    "        #output = nn.Softmax(dim=1)(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c98c40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c198fcb9",
   "metadata": {},
   "source": [
    "## Train - Valid Loop\n",
    "\n",
    "Fluxo de treinamento\n",
    "\n",
    "Input: modelo, dataloader_train, dataloader_teste , epocas.\n",
    "- Para cada epoca\n",
    "\n",
    "    - treinamento:\n",
    "\n",
    "        - para cada batch (lote de imagens) no dataloader_train: (executo para todo o dataloader)\n",
    "            \n",
    "            leitura das imagens e rotulos. \n",
    "            \n",
    "            prediz o rotulo (outputs): rotulos preditos para o tamanho do batch (ex: para um batch de 10, o \n",
    "            output tem tamanho 10)\n",
    "            \n",
    "            calcula o Loss () \n",
    "            \n",
    "            aplica o backward -> Ajuste dos pesos nos neuronios de acordo com o valor de loss (w_{i})\n",
    "\n",
    "            otimizo o modelo com optimizer.step()\n",
    "\n",
    "    - validacao (avaliacao do desempenho para aquele conjunto de treinamento):\n",
    "        - para cada batch (lote de imagens) no dataloader_train: (executo para todo o dataloader)\n",
    "\n",
    "            leitura das imagens e rotulos.\n",
    "\n",
    "            prediz o rotulo (outputs): rotulos preditos para o tamanho do batch (ex: para um batch de 10, o \n",
    "            output tem tamanho 10)\n",
    "            \n",
    "            calcula o Loss ()\n",
    "\n",
    "\n",
    "    - Calculo de metricas de loss media de validacao.\n",
    "\n",
    "    - Calculo de metrica de loss media de treinamento. \n",
    "\n",
    "    - Print de metricas de desempenho para a epoca. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "809e6d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_model(model, path = 'output/modelos', name_file='model.pth'):\n",
    "    full_path = os.path.join(path, name_file)\n",
    "    torch.save(model.state_dict(), full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60622c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_metricas(path='output/metricas', name_file_train='train_loss_total.npy', name_file_val='val_loss_total.npy',predict_label=None, true_label=None):\n",
    "    full_path_train = os.path.join(path, name_file_train)\n",
    "    full_path_val = os.path.join(path, name_file_val)\n",
    "    np.save(full_path_train, np.array(predict_label))\n",
    "    np.save(full_path_val, np.array(true_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5d5e01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \" + str(device))\n",
    "def simple_loop(model, train_image, val_image, epochs):\n",
    "    # Simple training loop\n",
    "    num_epochs = epochs\n",
    "    train_losses, val_losses = [], []\n",
    "    lim_loss = 1.5\n",
    "    iter_size = 5\n",
    "    print(f'Number of training images per iteration: {iter_size}')\n",
    "    #model = modelo( num_classes=5)\n",
    "    model.to(device)\n",
    "    criterion =  nn.L1Loss()\n",
    "    #criterion =  nn.CrossEntropyLoss()\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    predict_label_full_epoch = []\n",
    "    predict_label_full = []\n",
    "    true_label_full_epoch = []\n",
    "    true_label_full = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss_train = 0.0\n",
    "        for images, labels in tqdm(train_image, desc='Training loop'):\n",
    "            # Move inputs and labels to the device\n",
    "            images = images.to(torch.float)\n",
    "            image, label = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(image)\n",
    "            #print(outputs)\n",
    "            #print(label)\n",
    "            loss_train = criterion(outputs.float(), label.float())\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss_train.item() * label.size(0)\n",
    "        train_loss = running_loss_train / len(train_image.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        model.eval()\n",
    "        running_loss_valid = 0.0\n",
    "        rotulos =[] \n",
    "        predict_label =[]\n",
    "        true_label =[]\n",
    "        _iter=0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_image, desc='Validation loop'):\n",
    "                # Move inputs and labels to the device\n",
    "                images = images.to(torch.float)\n",
    "                images, label = images.to(device), labels.to(device)\n",
    "                rotulos.append(label.cpu().data.numpy())\n",
    "                outputs = model(images)\n",
    "                loss_valid = criterion(outputs.float(), label.float())\n",
    "                #print( [outputs.cpu().data.numpy().astype(int).T[0]])\n",
    "                #print(label.cpu().data.numpy().astype(int).T[0])\n",
    "                #print(predict_label)\n",
    "                try:\n",
    "                    _pred = outputs.cpu().data.numpy().astype(int).T[0].tolist()\n",
    "                    if(len(_pred) == iter_size):\n",
    "                        predict_label.append(_pred)\n",
    "                        _true = label.cpu().data.numpy().astype(int).T[0].tolist()\n",
    "                        true_label.append(_true)\n",
    "                except Exception as e:\n",
    "                    print(f\"Concatenation error iter: {e}\")\n",
    "                    print(_pred)\n",
    "                    print(predict_label)    \n",
    "                running_loss_valid += loss_valid.item() * label.size(0)\n",
    "                _iter +=1\n",
    "        val_loss = running_loss_valid / len(val_image.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f'End validation for epoch {epoch}')\n",
    "        print(f'Amount of images validated: {val_image}')\n",
    "        print(f'Label predict shape : {len(predict_label)} for epoch {epoch}')\n",
    "        print(f'Count of iterations: {_iter} for epoch {epoch}')\n",
    "        try:\n",
    "            _p = predict_label\n",
    "            _t = true_label\n",
    "            predict_label_full.append(_p)\n",
    "            true_label_full.append(_t)\n",
    "        except Exception as e:\n",
    "            print(f\"Concatenation error full: {e}\")\n",
    "            print(predict_label)\n",
    "            print(true_label)\n",
    "        #val_acc = accuracy_score(rotulos,output_model)\n",
    "        print(f'Val accuracy {epoch}:')\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss}, Validation loss: {val_loss}\")\n",
    "\n",
    "    predict_label_full_out = np.array(predict_label_full)\n",
    "    true_label_full_out = np.array(true_label_full)\n",
    "    salvar_metricas(path='output/metricas', name_file_train=f'predict_label_{i}.npy', name_file_val=f'true_label_{i}.npy', predict_label= outputs, true_label= labels)\n",
    "\n",
    "    return train_losses, val_losses, model,predict_label_full_out, true_label_full_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df8a989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "Train and valid for Fold 0\n",
      "Number of training images per iteration: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7acf9af80a467e8984cdf7a1bedac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c1e19a4d6c48a7b7b4ae6f03f306ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 0\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b302779d0>\n",
      "Label predict shape : 8 for epoch 0\n",
      "Count of iterations: 9 for epoch 0\n",
      "Val accuracy 0:\n",
      "Epoch 1/5 - Train loss: 0.6547354110624901, Validation loss: 5882.8451149680395\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a1b4430f864438bb229696178893a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397de4e7a13a45a1ab31d460516b470b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 1\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b302779d0>\n",
      "Label predict shape : 8 for epoch 1\n",
      "Count of iterations: 9 for epoch 1\n",
      "Val accuracy 1:\n",
      "Epoch 2/5 - Train loss: 0.5139306212320577, Validation loss: 8.281210774725134\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2972daf2311f4c2e9e1982286f309cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c80363339b45279c92fde6254039e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 2\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b302779d0>\n",
      "Label predict shape : 8 for epoch 2\n",
      "Count of iterations: 9 for epoch 2\n",
      "Val accuracy 2:\n",
      "Epoch 3/5 - Train loss: 0.4734740055621017, Validation loss: 0.7346589924259619\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4abd1c133754b579d8ecdc847eae6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9d43205c954d0eafb7c514b64810c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 3\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b302779d0>\n",
      "Label predict shape : 8 for epoch 3\n",
      "Count of iterations: 9 for epoch 3\n",
      "Val accuracy 3:\n",
      "Epoch 4/5 - Train loss: 0.4926266521215439, Validation loss: 4.090355324474248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4061562548994773909a4672dab59eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a2bd12235146feace7e02a1d4bd426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 4\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b302779d0>\n",
      "Label predict shape : 8 for epoch 4\n",
      "Count of iterations: 9 for epoch 4\n",
      "Val accuracy 4:\n",
      "Epoch 5/5 - Train loss: 0.5191400156811227, Validation loss: 3.9404308166016233\n",
      "Fold 1:\n",
      "Train and valid for Fold 1\n",
      "Number of training images per iteration: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231b26af362949569f68c18deda90402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3801d20f7fdc423dbb7bc858ed61941a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 0\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b3013a9e0>\n",
      "Label predict shape : 8 for epoch 0\n",
      "Count of iterations: 9 for epoch 0\n",
      "Val accuracy 0:\n",
      "Epoch 1/5 - Train loss: 0.7093354290448173, Validation loss: 899.9402636150982\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75453d4ed74842f69772506410aad9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09bb7edd72c4874b36e1ef7290471d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 1\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b3013a9e0>\n",
      "Label predict shape : 8 for epoch 1\n",
      "Count of iterations: 9 for epoch 1\n",
      "Val accuracy 1:\n",
      "Epoch 2/5 - Train loss: 0.525048672119317, Validation loss: 5.394087946692179\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f14307253440488e125696e52b15a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09791b7b75748a1b91023b0444b3d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 2\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b3013a9e0>\n",
      "Label predict shape : 8 for epoch 2\n",
      "Count of iterations: 9 for epoch 2\n",
      "Val accuracy 2:\n",
      "Epoch 3/5 - Train loss: 0.5397088203816055, Validation loss: 0.4885968898617944\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1569c92c87e541a3a16d6cb8f1b67b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda423db7b49418587257867e72ebbfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 3\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b3013a9e0>\n",
      "Label predict shape : 8 for epoch 3\n",
      "Count of iterations: 9 for epoch 3\n",
      "Val accuracy 3:\n",
      "Epoch 4/5 - Train loss: 0.528195950423362, Validation loss: 0.47013208477996116\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69fd8816eb8a42da80421e627cfb8d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad73c8ae89d4ef0ac1d3135512e6e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 4\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b3013a9e0>\n",
      "Label predict shape : 8 for epoch 4\n",
      "Count of iterations: 9 for epoch 4\n",
      "Val accuracy 4:\n",
      "Epoch 5/5 - Train loss: 0.4947295043337552, Validation loss: 0.9448888086995413\n",
      "Fold 2:\n",
      "Train and valid for Fold 2\n",
      "Number of training images per iteration: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360d61d540c54a26a9c84496d227c687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1be7e212db4a7795e9454090e3a9a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 0\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b30143130>\n",
      "Label predict shape : 8 for epoch 0\n",
      "Count of iterations: 9 for epoch 0\n",
      "Val accuracy 0:\n",
      "Epoch 1/5 - Train loss: 0.7013152431201384, Validation loss: 119.64244221532067\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02608009e3444d50bf77511139c72752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792a7ecf9f6f4819afbec479013ccdca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 1\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b30143130>\n",
      "Label predict shape : 8 for epoch 1\n",
      "Count of iterations: 9 for epoch 1\n",
      "Val accuracy 1:\n",
      "Epoch 2/5 - Train loss: 0.5031956514731222, Validation loss: 0.5798953265644783\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0992e7ece34029a6911d5f51b0aca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ced6951d6744a3396721f9923471578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 2\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b30143130>\n",
      "Label predict shape : 8 for epoch 2\n",
      "Count of iterations: 9 for epoch 2\n",
      "Val accuracy 2:\n",
      "Epoch 3/5 - Train loss: 0.5006326620030954, Validation loss: 14.334582484045693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3971da31a3476988689e3cd15b6154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1349bd0f3e9741018a21e09bb788092a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 3\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b30143130>\n",
      "Label predict shape : 8 for epoch 3\n",
      "Count of iterations: 9 for epoch 3\n",
      "Val accuracy 3:\n",
      "Epoch 4/5 - Train loss: 0.5059408061421675, Validation loss: 6.179135078607604\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7e05426eac47aa8026edbed0cbda07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513569e6fbe34f58bc95c681e9400d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 4\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b30143130>\n",
      "Label predict shape : 8 for epoch 4\n",
      "Count of iterations: 9 for epoch 4\n",
      "Val accuracy 4:\n",
      "Epoch 5/5 - Train loss: 0.5004143853580332, Validation loss: 7.377310398013093\n",
      "Fold 3:\n",
      "Train and valid for Fold 3\n",
      "Number of training images per iteration: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aed83883b8f4ca5a5365ed05c0d5f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9865450d87146aaa7f51f84e62df197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 0\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b30286050>\n",
      "Label predict shape : 8 for epoch 0\n",
      "Count of iterations: 9 for epoch 0\n",
      "Val accuracy 0:\n",
      "Epoch 1/5 - Train loss: 0.6471147905023111, Validation loss: 112.94817813607149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f756959e6a4742be81b57ed06e45f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199f399725af463fb122b9f3537f1f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 1\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b30286050>\n",
      "Label predict shape : 8 for epoch 1\n",
      "Count of iterations: 9 for epoch 1\n",
      "Val accuracy 1:\n",
      "Epoch 2/5 - Train loss: 0.5459410274304406, Validation loss: 89.9577803500863\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1547db4b700e4ae3aa0084b0a3eec2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bd86687c11483fb76608cb854d49d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 2\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b30286050>\n",
      "Label predict shape : 8 for epoch 2\n",
      "Count of iterations: 9 for epoch 2\n",
      "Val accuracy 2:\n",
      "Epoch 3/5 - Train loss: 0.5167344815038533, Validation loss: 8.814542903456577\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb0e2b74d004c289678cc2c7ca7b4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4b8b07447c470ebbf930acfd3eb3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 3\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b30286050>\n",
      "Label predict shape : 8 for epoch 3\n",
      "Count of iterations: 9 for epoch 3\n",
      "Val accuracy 3:\n",
      "Epoch 4/5 - Train loss: 0.5360960470286408, Validation loss: 0.5226627823918365\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bf05e3e6644f3d8c29df4f81811947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d009fcd4e5446a4aa6eaedd9468b5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 4\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b30286050>\n",
      "Label predict shape : 8 for epoch 4\n",
      "Count of iterations: 9 for epoch 4\n",
      "Val accuracy 4:\n",
      "Epoch 5/5 - Train loss: 0.5191355432044564, Validation loss: 2.038539916970009\n",
      "Fold 4:\n",
      "Train and valid for Fold 4\n",
      "Number of training images per iteration: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9124f449ce2349cf9a8bc61f533c5a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6484adb1751f4e9d8d621ffc9b5d3404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 0\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b30287050>\n",
      "Label predict shape : 8 for epoch 0\n",
      "Count of iterations: 9 for epoch 0\n",
      "Val accuracy 0:\n",
      "Epoch 1/5 - Train loss: 0.5806318329421082, Validation loss: 87669.81013808139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2300772e0ac4e2d971b06d8af239f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f57f7a276443d7899f997c206df881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 1\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b30287050>\n",
      "Label predict shape : 8 for epoch 1\n",
      "Count of iterations: 9 for epoch 1\n",
      "Val accuracy 1:\n",
      "Epoch 2/5 - Train loss: 0.5859994053668369, Validation loss: 0.4257378982769888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4724f4816a047b8b3ca1914f3e5b33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7506bd57780745d7a5811487d1ca2ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 2\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b30287050>\n",
      "Label predict shape : 8 for epoch 2\n",
      "Count of iterations: 9 for epoch 2\n",
      "Val accuracy 2:\n",
      "Epoch 3/5 - Train loss: 0.57526360271294, Validation loss: 33.293586109959804\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe6db0332174e76aa2d4f68461ca913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a69300af7d64b8faf7bb47a0b39cdeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 3\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b30287050>\n",
      "Label predict shape : 8 for epoch 3\n",
      "Count of iterations: 9 for epoch 3\n",
      "Val accuracy 3:\n",
      "Epoch 4/5 - Train loss: 0.5057160468459818, Validation loss: 3.476217269897461\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9765e5336534695b9b98bfef9e49076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc7bc1e33784b289c2b9319e0312903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 4\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x7c7b30287050>\n",
      "Label predict shape : 8 for epoch 4\n",
      "Count of iterations: 9 for epoch 4\n",
      "Val accuracy 4:\n",
      "Epoch 5/5 - Train loss: 0.5069192563522757, Validation loss: 0.4343366946938426\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loss_total = []\n",
    "val_loss_total =[]\n",
    "all_models =[]\n",
    "epochs = 5\n",
    "## create first model.\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train_dataset)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    #print(f\"  Train: index={train_index}\")\n",
    "    #print(f\"  Test:  index={test_index}\")\n",
    "    ## init train test for folder\n",
    "    train_dataset_part = Subset( train_dataset, train_index)\n",
    "    val_dataset_part = Subset( train_dataset, test_index)\n",
    "\n",
    "    train_loader_img = DataLoader(train_dataset_part, batch_size=5, shuffle=True)\n",
    "    val_loader_img = DataLoader(val_dataset_part, batch_size=5, shuffle=True)\n",
    "\n",
    "    model= ECGClassifierResnet( num_classes=1)\n",
    "    salvar_model(model, path='output/modelos', name_file=f'model_fold_{i}.pth')\n",
    "    print(f'Train and valid for Fold {i}')\n",
    "    t, l,_,outputs,labels = simple_loop(model, train_loader_img,val_loader_img, epochs)\n",
    "    ## Evaluate model.\n",
    "    train_loss_total.append(t)\n",
    "    val_loss_total.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "edc2494b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616],\n",
       "        [31616, 31616, 31616, 31616, 31616]],\n",
       "\n",
       "       [[   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17],\n",
       "        [   17,    17,    17,    17,    17]],\n",
       "\n",
       "       [[    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2],\n",
       "        [    2,     2,     2,     2,     2]],\n",
       "\n",
       "       [[    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0]],\n",
       "\n",
       "       [[    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs\n",
    "## formato ddos outputs [epoch][batch][labels]\n",
    "#out = np.array(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38154859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0],\n",
       "       [1, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels[0][0:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345f4f10",
   "metadata": {},
   "source": [
    "## Métricas de Avaliacao dos modelos\n",
    "\n",
    "Acuracia\n",
    "\n",
    "Precisao\n",
    "\n",
    "Sensibilidade\n",
    "\n",
    "Especificidade\n",
    "\n",
    "F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "70a0dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Leitura de métricas salvas\n",
    "import numpy as np\n",
    "predict_label_0 = np.load('output/metricas/predict_label_0.npy')\n",
    "true_label_0 = np.load('output/metricas/true_label_0.npy')\n",
    "# --- IGNORE ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1251295b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[5090, 4552, 5090, 5090, 5090],\n",
       "        [9356, 5090, 3771, 5021, 5090],\n",
       "        [5090, 5090, 3520, 5090, 5453],\n",
       "        [5090, 5090, 5090, 9419, 5090],\n",
       "        [5090, 5090, 5090, 5090, 4520],\n",
       "        [5090, 5090, 4277, 5090, 5090],\n",
       "        [3335, 5090, 7920, 5090, 5090],\n",
       "        [5090, 5090, 5017, 5281, 5090]],\n",
       "\n",
       "       [[   0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    0]],\n",
       "\n",
       "       [[  31,   31,    1,    3,   31],\n",
       "        [  31,   31,   31,    1,   31],\n",
       "        [  31,   31,    4,   31,   31],\n",
       "        [  31,   31,    2,   31,   31],\n",
       "        [  20,   31,   31,    0,   31],\n",
       "        [  77,   11,   31,   31,   31],\n",
       "        [  31,    4,   31,    1,   31],\n",
       "        [   2,   31,   39,    9,   31]],\n",
       "\n",
       "       [[   0,    1,    1,    1,    1],\n",
       "        [   1,    1,    1,    0,    1],\n",
       "        [   1,    1,    1,    1,    1],\n",
       "        [   0,    0,    0,    0,    1],\n",
       "        [   1,    1,    0,    1,    1],\n",
       "        [   1,    0,    1,    3,    1],\n",
       "        [   1,    1,    0,    0,    0],\n",
       "        [  13,    1,    0,    0,    1]],\n",
       "\n",
       "       [[   1,    1,    1,    0,    1],\n",
       "        [   1,    1,    0,    1,    0],\n",
       "        [   1,    1,    0,    1,    0],\n",
       "        [   1,    1,    0,    0,    1],\n",
       "        [   0,    1,    1,    1,    1],\n",
       "        [   1,    1,    0,    1,    1],\n",
       "        [   0,    1,    1,    1,    0],\n",
       "        [   0,    1,    0,    1,    1]]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd73f641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.75"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "220/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9399a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(102, 5)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"a\")\n",
    "print(type(predict_label_0))\n",
    "predict_label_0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc67921",
   "metadata": {},
   "source": [
    "## Criacao graficos de treinamento e validacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f25929",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Criar graficos de treinamento e validacao\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(5):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(train_loss_total[i], label='Train Loss')\n",
    "    plt.plot(val_loss_total[i], label='Validation Loss')\n",
    "    plt.title(f'Fold {i} - Train and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'output/graficos/loss_fold_{i}.png')\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fdd0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

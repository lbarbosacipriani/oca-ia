{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beb18702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (12.1.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (2.4.2)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (2.10.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 5)) (1.8.0)\n",
      "Requirement already satisfied: timm in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 6)) (1.0.24)\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 7)) (8.1.8)\n",
      "Requirement already satisfied: widgetsnbextension in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 8)) (4.0.15)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 9)) (3.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (80.10.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (2026.1.0)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 4)) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in ./.venv/lib/python3.13/site-packages (from cuda-bindings==12.9.4->torch->-r requirements.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: scipy>=1.10.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.13/site-packages (from timm->-r requirements.txt (line 6)) (0.25.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.13/site-packages (from timm->-r requirements.txt (line 6)) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.13/site-packages (from timm->-r requirements.txt (line 6)) (1.3.5)\n",
      "Requirement already satisfied: safetensors in ./.venv/lib/python3.13/site-packages (from timm->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.13/site-packages (from ipywidgets->-r requirements.txt (line 7)) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.13/site-packages (from ipywidgets->-r requirements.txt (line 7)) (9.9.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.13/site-packages (from ipywidgets->-r requirements.txt (line 7)) (5.14.3)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./.venv/lib/python3.13/site-packages (from ipywidgets->-r requirements.txt (line 7)) (3.0.16)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 9)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 9)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 9)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 9)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 9)) (26.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./.venv/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 9)) (3.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.2 in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.5.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.13/site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.13/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.13/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.13/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in ./.venv/lib/python3.13/site-packages (from huggingface_hub->timm->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from huggingface_hub->timm->-r requirements.txt (line 6)) (0.28.1)\n",
      "Requirement already satisfied: shellingham in ./.venv/lib/python3.13/site-packages (from huggingface_hub->timm->-r requirements.txt (line 6)) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.13/site-packages (from huggingface_hub->timm->-r requirements.txt (line 6)) (4.67.2)\n",
      "Requirement already satisfied: typer-slim in ./.venv/lib/python3.13/site-packages (from huggingface_hub->timm->-r requirements.txt (line 6)) (0.21.1)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm->-r requirements.txt (line 6)) (4.12.1)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm->-r requirements.txt (line 6)) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm->-r requirements.txt (line 6)) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm->-r requirements.txt (line 6)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub->timm->-r requirements.txt (line 6)) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch->-r requirements.txt (line 4)) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.13/site-packages (from typer-slim->huggingface_hub->timm->-r requirements.txt (line 6)) (8.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b40d20cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image, ImageChops\n",
    "import numpy as np\n",
    "import torch\n",
    "from lib.ImageFIlter import treat_image_PIL\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import  DataLoader, TensorDataset, Dataset\n",
    "from torch import nn\n",
    "import timm\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de731cfa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0662af5c",
   "metadata": {},
   "source": [
    "## Funcao de filtro de imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25891a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageChops\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import io\n",
    "from pathlib import Path\n",
    "path = '/home/leo/Documents/ecg_classifier/dataset/database_ptbxl/'\n",
    "def treat_image_PIL(img_path, type_return=2):\n",
    "    ''''\n",
    "    Input: Img_path, type return.\n",
    "\n",
    "    Img_path: path da imagem em formato png, img...\n",
    "    type_return: 1-> retorno como PIL. \n",
    "                2 ou sem type_return -> retorno como numpy array tipo uint8\n",
    "\n",
    "    Output:\n",
    "    '''\n",
    "    im = Image.open(path+img_path) \n",
    "    \n",
    "    # Size of the image in pixels (size of original image) \n",
    "    # (This is not mandatory) \n",
    "    width, height = im.size \n",
    "\n",
    "    rgb =Image.Image.split(im) \n",
    "\n",
    "    data =rgb\n",
    "    b= data[0]\n",
    "    g= data[1]\n",
    "    r= data[2]\n",
    "    #img_out = b+g+.5*r\n",
    "    #img_out_2 = img_out[500:1600, 50:2100] \n",
    "\n",
    "    newsize = (256, 256)\n",
    "   # im3 =ImageChops.subtract(mask,b, scale=1.0, offset=0)\n",
    "\n",
    "    b1 = b.crop((120,500,2100,1600))\n",
    "    g1 = g.crop((120,500,2100,1600))\n",
    "    r1 = r.crop((120,500,2100,1600))\n",
    "    im1 = b1.resize(newsize, Image.Resampling.LANCZOS).convert('L')\n",
    "    im2 = g1.resize(newsize, Image.Resampling.LANCZOS).convert('L')\n",
    "    im3 = r1.resize(newsize, Image.Resampling.LANCZOS).convert('L')\n",
    "    if type_return ==1:\n",
    "        return im1 \n",
    "    else:\n",
    "        return np.array([im1,im2,im3],dtype=np.uint8)\n",
    "\n",
    "\n",
    "def save_file_to_dir(file_obj, directory, filename):\n",
    "    \"\"\"\n",
    "    Save a file to `directory` with the given `filename`.\n",
    "\n",
    "    Parameters\n",
    "    - file_obj: a PIL Image, a filesystem path (str or Path) to an existing file,\n",
    "                bytes/bytearray, or a file-like object with a .read() method.\n",
    "    - directory: target directory where the file will be saved.\n",
    "    - filename: the name to use for the saved file (including extension if desired).\n",
    "\n",
    "    Returns\n",
    "    - full path (str) to the saved file.\n",
    "\n",
    "    Raises\n",
    "    - FileNotFoundError if a provided source path does not exist.\n",
    "    - ValueError if the provided file_obj type is unsupported.\n",
    "    \"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    dest = os.path.join(directory, filename)\n",
    "\n",
    "    # PIL Image\n",
    "    if isinstance(file_obj, Image.Image):\n",
    "        file_obj.save(dest)\n",
    "        return dest\n",
    "\n",
    "    # Path-like or string pointing to an existing file\n",
    "    if isinstance(file_obj, (str, Path)):\n",
    "        src = str(file_obj)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dest)\n",
    "            return dest\n",
    "        raise FileNotFoundError(f\"Source path not found: {src}\")\n",
    "\n",
    "    # File-like object\n",
    "    if hasattr(file_obj, \"read\"):\n",
    "        data = file_obj.read()\n",
    "        # If read() returned bytes -> try open as image, otherwise write raw\n",
    "        if isinstance(data, (bytes, bytearray)):\n",
    "            try:\n",
    "                img = Image.open(io.BytesIO(data))\n",
    "                img.save(dest)\n",
    "                return dest\n",
    "            except Exception:\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    f.write(data)\n",
    "                return dest\n",
    "        else:\n",
    "            # assume text\n",
    "            with open(dest, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(data)\n",
    "            return dest\n",
    "\n",
    "    # Raw bytes\n",
    "    if isinstance(file_obj, (bytes, bytearray)):\n",
    "        try:\n",
    "            img = Image.open(io.BytesIO(file_obj))\n",
    "            img.save(dest)\n",
    "            return dest\n",
    "        except Exception:\n",
    "            with open(dest, \"wb\") as f:\n",
    "                f.write(file_obj)\n",
    "            return dest\n",
    "\n",
    "    raise ValueError(\"file_obj must be a PIL.Image, path string/Path, bytes or file-like object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83d7ce4",
   "metadata": {},
   "source": [
    "## Funcoes output_metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44fde85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "## Cria funcao para validar se pasta a ser inserida existe. Caso nao exista, cria a pasta\n",
    "def create_folder_if_not_exists(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f'Pasta {folder_path} criada.')\n",
    "    else:\n",
    "        print(f'Pasta {folder_path} ja existe.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63dfac",
   "metadata": {},
   "source": [
    "## Funcoes subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3057848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import  Dataset\n",
    "\n",
    "class Subset(Dataset):\n",
    "    r\"\"\"\n",
    "    Subset of a dataset at specified indices.\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): The whole Dataset\n",
    "        indices (sequence): Indices in the whole set selected for subset\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    @property\n",
    "    def classes(self):\n",
    "        return self.data.classes\n",
    "\n",
    "    def shape(self):\n",
    "        return self.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a35fdff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \" + str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f255a4",
   "metadata": {},
   "source": [
    "## Funcoes simple_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9688a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \" + str(device))\n",
    "\n",
    "def salvar_model(model, path = 'output/modelos', name_file='model.pth'):\n",
    "    create_folder_if_not_exists(path)\n",
    "\n",
    "    full_path = os.path.join(path, name_file)\n",
    "    torch.save(model.state_dict(), full_path)\n",
    "\n",
    "def salvar_metricas(path, name_file_train='train_loss_total.npy', name_file_val='val_loss_total.npy',predict_label=None, true_label=None):\n",
    "    create_folder_if_not_exists(path)\n",
    "    full_path_train = os.path.join(path, name_file_train)\n",
    "    full_path_val = os.path.join(path, name_file_val)\n",
    "    np.save(full_path_train, np.array(predict_label))\n",
    "    np.save(full_path_val, np.array(true_label))\n",
    "    print(f'Metricas salvas em {path} com os nomes {name_file_train} e {name_file_val} e tamanhos {np.array(predict_label).shape} e {np.array(true_label).shape}')\n",
    "\n",
    "def simple_loop(model, train_image, val_image, epochs, batch_size, fold_index):\n",
    "    # Simple training loop\n",
    "    num_epochs = epochs\n",
    "    train_losses, val_losses = [], []\n",
    "    lim_loss = 1.5\n",
    "    iter_size = batch_size\n",
    "    print(f'Number of training images per iteration: {iter_size}')\n",
    "    #model = modelo( num_classes=5)\n",
    "  # model.to(device)\n",
    "    #criterion =  nn.NLLLoss()\n",
    "    criterion =  nn.BCEWithLogitsLoss()\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    predict_label_full_train = []\n",
    "    predict_label_full = []\n",
    "    true_label_full_train = []\n",
    "    true_label_full = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        predict_label_train =[]\n",
    "        true_label_train =[]\n",
    "        running_loss_train = 0.0\n",
    "        for images, labels in tqdm(train_image, desc='Training loop'):\n",
    "            # Move inputs and labels to the device\n",
    "            images = images.to(torch.float)\n",
    "            image, label = images, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(image)\n",
    "            #loss_train = criterion(outputs.float(), label.float())\n",
    "            loss_train = criterion(outputs, label)\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss_train.item() * label.size(0)\n",
    "            try:\n",
    "                _pred_train = outputs.cpu().data.numpy().astype(int).T[0].tolist()\n",
    "                if(len(_pred_train) == iter_size):\n",
    "                    predict_label_train.append(_pred_train)\n",
    "                    _true_train = label.cpu().data.numpy().astype(int).T[0].tolist()\n",
    "                    true_label_train.append(_true_train)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Concatenation error iter: {e}\")\n",
    "                print(_pred_train)\n",
    "                print(predict_label_train)    \n",
    "        train_loss = running_loss_train / len(train_image.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        try:\n",
    "            _p_train = predict_label_train\n",
    "            _t_train = true_label_train\n",
    "            predict_label_full_train.append(_p_train)\n",
    "            true_label_full_train.append(_t_train)\n",
    "        except Exception as e:\n",
    "            print(f\"Concatenation error full: {e}\")\n",
    "            print(predict_label_train)\n",
    "            print(true_label_train)\n",
    "        model.eval()\n",
    "        running_loss_valid = 0.0\n",
    "        rotulos =[] \n",
    "        predict_label =[]\n",
    "        true_label =[]\n",
    "        _iter=0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_image, desc='Validation loop'):\n",
    "                # Move inputs and labels to the device\n",
    "                images = images.to(torch.float)\n",
    "                images, label = images, labels\n",
    "                rotulos.append(label.cpu().data.numpy())\n",
    "                outputs = model(images)\n",
    "                #loss_valid = criterion(outputs.float(), label.float())\n",
    "                loss_valid = criterion(outputs, label)\n",
    "\n",
    "                #print( [outputs.cpu().data.numpy().astype(int).T[0]])\n",
    "                #print(label.cpu().data.numpy().astype(int).T[0])\n",
    "                #print(predict_label)\n",
    "                try:\n",
    "                    _pred = outputs.cpu().data.numpy().astype(int).T[0].tolist()\n",
    "\n",
    "                    if(len(_pred) == iter_size):\n",
    "                        predict_label.append(_pred)\n",
    "                        _true = label.cpu().data.numpy().astype(int).T[0].tolist()\n",
    "                        true_label.append(_true)\n",
    "                except Exception as e:\n",
    "                    print(f\"Concatenation error iter: {e}\")\n",
    "                    print(_pred)\n",
    "                    print(predict_label)    \n",
    "                running_loss_valid += loss_valid.item() * label.size(0)\n",
    "                _iter +=1\n",
    "        val_loss = running_loss_valid / len(val_image.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f'End validation for epoch {epoch}')\n",
    "        print(f'Amount of images validated: {val_image}')\n",
    "        print(f'Label predict shape : {len(predict_label)} for epoch {epoch}')\n",
    "        print(f'Count of iterations: {_iter} for epoch {epoch}')\n",
    "        try:\n",
    "            _p = predict_label\n",
    "            _t = true_label\n",
    "            predict_label_full.append(_p)\n",
    "            true_label_full.append(_t)\n",
    "        except Exception as e:\n",
    "            print(f\"Concatenation error full: {e}\")\n",
    "            print(predict_label)\n",
    "            print(true_label)\n",
    "        #val_acc = accuracy_score(rotulos,output_model)\n",
    "        print(f'Val accuracy {epoch}:')\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss}, Validation loss: {val_loss}\")\n",
    "\n",
    "    predict_label_full_out = np.array(predict_label_full)\n",
    "    true_label_full_out = np.array(true_label_full)\n",
    "    predict_label_full_train_out = np.array(predict_label_full_train)\n",
    "    true_label_full_train_out = np.array(true_label_full_train)\n",
    "    print(f'Salvado das metricas de validacao e treino')\n",
    "    salvar_metricas(path=f'output/metricas/valid/fold_{fold_index}', name_file_train=f'predict_label_valid_fold_{fold_index}.npy', name_file_val=f'true_label_valid_fold_{fold_index}.npy', predict_label= predict_label_full_out, true_label= true_label_full_out)\n",
    "    salvar_metricas(path=f'output/metricas/train/fold_{fold_index}', name_file_train=f'predict_label_train_fold_{fold_index}.npy', name_file_val=f'true_label_train_fold_{fold_index}.npy', predict_label= predict_label_full_train_out, true_label= true_label_full_train_out)\n",
    "    print(f'Finalizado o salvamento das metricas')\n",
    "    return train_losses, val_losses, model,predict_label_full_out, true_label_full_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb6c0f",
   "metadata": {},
   "source": [
    "## Funcao Modelo ECG Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "155cbfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models import ECGClassifierResnet\n",
    "from torch import nn\n",
    "import timm\n",
    "class ECGClassifierResnet(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(ECGClassifierResnet, self).__init__()\n",
    "        # Where we define all the parts of the model\n",
    "        #self.base_model = timm.create_model('efficientnet_b0', pretrained=True) \n",
    "        self.base_model=timm.create_model('resnet50d.ra4_e3600_r224_in1k',pretrained=True)\n",
    "        #self.base_model = timm.create_model('vit_mediumd_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k',num_classes=5,pretrained=True)\n",
    "\n",
    "\n",
    "        self.features = nn.Sequential(*list(self.base_model.children())[:-1])\n",
    "\n",
    "        enet_out_size = 2048        # Make a classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(enet_out_size,1)\n",
    "        ) # saida como Sigmoid multilabel\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Connect these parts and return the output\n",
    "        x = self.features(x)\n",
    "        output = self.classifier(x)\n",
    "        #output = nn.Softmax(dim=1)(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee1a7a3",
   "metadata": {},
   "source": [
    "## Arquivo Main.py - Orquestrador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd08e02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o programa...\n",
      "Parametros de Execucao:\n",
      "  Numero de folds para K-Fold Cross Validation: 5\n",
      "  Tamanho do batch para treinamento: 10\n",
      "  Numero de epocas para treinamento: 20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FILE_PATH = 'norm_outros_dataset.csv'\n",
    "\n",
    "\n",
    "print(\"Iniciando o programa...\")\n",
    "print(\"Parametros de Execucao:\")\n",
    "folds = 5\n",
    "epochs = 20\n",
    "BATCH_SIZE = 10\n",
    "flg_salvar_modelos = True\n",
    "\n",
    "print(f\"  Numero de folds para K-Fold Cross Validation: {folds}\")\n",
    "\n",
    "print(f\"  Tamanho do batch para treinamento: {BATCH_SIZE}\")\n",
    "\n",
    "print(f\"  Numero de epocas para treinamento: {epochs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33797eae",
   "metadata": {},
   "source": [
    "### Geracao do tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10c59064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leitura dos dados no arquivo 'norm_outros_dataset.csv'...\n",
      "Dados lidos com sucesso. Tamnaho dos dados: (500, 2)\n",
      "Geracao Tensor de Imagens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 500/500 [00:28<00:00, 17.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor de Imagens gerado com sucesso. Tamanho do Tensor: torch.Size([500, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"Leitura dos dados no arquivo '{FILE_PATH}'...\")\n",
    "\n",
    "data = pd.read_csv(FILE_PATH)\n",
    "data = data.sample(n=500, random_state=42).reset_index(drop=True)  # Embaralha e seleciona 2000 amostras\n",
    "data.rename(columns={data.columns[0]:'path'}, inplace=True)\n",
    "data.rename(columns={data.columns[1]:'label'}, inplace=True)\n",
    "print(\"Dados lidos com sucesso. Tamnaho dos dados:\", data.shape)\n",
    "\n",
    "print(\"Geracao Tensor de Imagens...\")\n",
    "## Loop\n",
    "img_dataset = np.ones((data.shape[0],3,256,256),dtype=np.uint8)\n",
    "\n",
    "j=0\n",
    "for i in tqdm(data['path'], desc='Processing images'):\n",
    "    img_dataset[j]=treat_image_PIL(i,2)\n",
    "    j+=1\n",
    "tensor_imagem = torch.tensor(img_dataset)\n",
    "print(\"Tensor de Imagens gerado com sucesso. Tamanho do Tensor:\", tensor_imagem.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95041878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Distribuicao das amostras\n",
    "## Histograma da distribuicao das amostras\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))  \n",
    "data['label'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribuição das Amostras por Classe')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Contagem de Amostras')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "648b005b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = tensor_imagem[0][0]\n",
    "len(img1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966dbdca",
   "metadata": {},
   "source": [
    "### Geracao de rótulos para classificação\n",
    "\n",
    "Colunas utilizadas para classificacao (Labels)\n",
    "\n",
    "labels = [ OCA, NOCA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29f9adfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor de Rotulos sendo gerado...\n",
      "Tensor de Rotulos gerado com sucesso. Tamanho do Tensor: torch.Size([500, 1])\n",
      "Configuracao do K-Fold para 5 folds...\n",
      "KFold(n_splits=5, random_state=None, shuffle=False)\n",
      "K-Fold configurado com sucesso.\n",
      "Iniciando o treinamento com K-Fold Cross Validation...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Tensor de Rotulos sendo gerado...\")\n",
    "\n",
    "\n",
    "tensor_label = torch.tensor(np.array(data['label'].astype(\"float32\"))).unsqueeze(1)\n",
    "print(\"Tensor de Rotulos gerado com sucesso. Tamanho do Tensor:\", tensor_label.shape)\n",
    "\n",
    "print(f\"Configuracao do K-Fold para {folds} folds...\")\n",
    "kf = KFold(n_splits=folds)\n",
    "kf.get_n_splits(tensor_imagem)\n",
    "print(kf)\n",
    "print(\"K-Fold configurado com sucesso.\")\n",
    "\n",
    "print(\"Iniciando o treinamento com K-Fold Cross Validation...\")\n",
    "\n",
    "train_loss_total = []\n",
    "val_loss_total =[]\n",
    "all_models =[]\n",
    "\n",
    "train_dataset = TensorDataset(tensor_imagem, tensor_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c198fcb9",
   "metadata": {},
   "source": [
    "## Train - Valid Loop\n",
    "\n",
    "Fluxo de treinamento\n",
    "\n",
    "Input: modelo, dataloader_train, dataloader_teste , epocas.\n",
    "- Para cada epoca\n",
    "\n",
    "    - treinamento:\n",
    "\n",
    "        - para cada batch (lote de imagens) no dataloader_train: (executo para todo o dataloader)\n",
    "            \n",
    "            leitura das imagens e rotulos. \n",
    "            \n",
    "            prediz o rotulo (outputs): rotulos preditos para o tamanho do batch (ex: para um batch de 10, o \n",
    "            output tem tamanho 10)\n",
    "            \n",
    "            calcula o Loss () \n",
    "            \n",
    "            aplica o backward -> Ajuste dos pesos nos neuronios de acordo com o valor de loss (w_{i})\n",
    "\n",
    "            otimizo o modelo com optimizer.step()\n",
    "\n",
    "    - validacao (avaliacao do desempenho para aquele conjunto de treinamento):\n",
    "        - para cada batch (lote de imagens) no dataloader_train: (executo para todo o dataloader)\n",
    "\n",
    "            leitura das imagens e rotulos.\n",
    "\n",
    "            prediz o rotulo (outputs): rotulos preditos para o tamanho do batch (ex: para um batch de 10, o \n",
    "            output tem tamanho 10)\n",
    "            \n",
    "            calcula o Loss ()\n",
    "\n",
    "\n",
    "    - Calculo de metricas de loss media de validacao.\n",
    "\n",
    "    - Calculo de metrica de loss media de treinamento. \n",
    "\n",
    "    - Print de metricas de desempenho para a epoca. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5d5e01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      ###############################################\n",
      "      Inicio do treinamento com K-Fold Cross Validation\n",
      "      ###############################################\n",
      "      \n",
      "Fold 0:\n",
      "Pasta output/modelos ja existe.\n",
      "Train and valid for Fold 0\n",
      "Number of training images per iteration: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:04<00:00,  1.61s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 0\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 0\n",
      "Count of iterations: 10 for epoch 0\n",
      "Val accuracy 0:\n",
      "Epoch 1/20 - Train loss: 0.6226688556373119, Validation loss: 0.6500192761421204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:04<00:00,  1.61s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 1\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 1\n",
      "Count of iterations: 10 for epoch 1\n",
      "Val accuracy 1:\n",
      "Epoch 2/20 - Train loss: 0.42333133667707445, Validation loss: 0.47255601584911344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.63s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 2\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 2\n",
      "Count of iterations: 10 for epoch 2\n",
      "Val accuracy 2:\n",
      "Epoch 3/20 - Train loss: 0.2817439349368215, Validation loss: 0.8266502618789673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.64s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 3\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 3\n",
      "Count of iterations: 10 for epoch 3\n",
      "Val accuracy 3:\n",
      "Epoch 4/20 - Train loss: 0.16920238304883242, Validation loss: 0.3177190013229847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.63s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 4\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 4\n",
      "Count of iterations: 10 for epoch 4\n",
      "Val accuracy 4:\n",
      "Epoch 5/20 - Train loss: 0.1668956853915006, Validation loss: 0.3651444710791111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:04<00:00,  1.61s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 5\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 5\n",
      "Count of iterations: 10 for epoch 5\n",
      "Val accuracy 5:\n",
      "Epoch 6/20 - Train loss: 0.10288798869587482, Validation loss: 0.6746343493461608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.63s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 6\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 6\n",
      "Count of iterations: 10 for epoch 6\n",
      "Val accuracy 6:\n",
      "Epoch 7/20 - Train loss: 0.1881891414988786, Validation loss: 0.7692739456892014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:04<00:00,  1.61s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 7\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 7\n",
      "Count of iterations: 10 for epoch 7\n",
      "Val accuracy 7:\n",
      "Epoch 8/20 - Train loss: 0.0954435046762228, Validation loss: 0.4083130367100239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:04<00:00,  1.60s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 8\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 8\n",
      "Count of iterations: 10 for epoch 8\n",
      "Val accuracy 8:\n",
      "Epoch 9/20 - Train loss: 0.0626867852639407, Validation loss: 0.5523463617544622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:03<00:00,  1.59s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 9\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 9\n",
      "Count of iterations: 10 for epoch 9\n",
      "Val accuracy 9:\n",
      "Epoch 10/20 - Train loss: 0.06132043356774375, Validation loss: 0.38904706239700315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:04<00:00,  1.60s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 10\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 10\n",
      "Count of iterations: 10 for epoch 10\n",
      "Val accuracy 10:\n",
      "Epoch 11/20 - Train loss: 0.07772338854847476, Validation loss: 0.4675698548555374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:04<00:00,  1.61s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 11\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 11\n",
      "Count of iterations: 10 for epoch 11\n",
      "Val accuracy 11:\n",
      "Epoch 12/20 - Train loss: 0.11305534420534968, Validation loss: 0.3102650910615921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:04<00:00,  1.61s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 12\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 12\n",
      "Count of iterations: 10 for epoch 12\n",
      "Val accuracy 12:\n",
      "Epoch 13/20 - Train loss: 0.12853765052277594, Validation loss: 0.5247785031795502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:04<00:00,  1.62s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 13\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 13\n",
      "Count of iterations: 10 for epoch 13\n",
      "Val accuracy 13:\n",
      "Epoch 14/20 - Train loss: 0.06455387582536787, Validation loss: 0.4705077961087227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:04<00:00,  1.61s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 14\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 14\n",
      "Count of iterations: 10 for epoch 14\n",
      "Val accuracy 14:\n",
      "Epoch 15/20 - Train loss: 0.07574830492958426, Validation loss: 0.39917593225836756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:03<00:00,  1.60s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 15\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 15\n",
      "Count of iterations: 10 for epoch 15\n",
      "Val accuracy 15:\n",
      "Epoch 16/20 - Train loss: 0.059681017376715315, Validation loss: 0.538015303760767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:03<00:00,  1.60s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 16\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 16\n",
      "Count of iterations: 10 for epoch 16\n",
      "Val accuracy 16:\n",
      "Epoch 17/20 - Train loss: 0.04154044054739643, Validation loss: 0.4300541512668133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:03<00:00,  1.60s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 17\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 17\n",
      "Count of iterations: 10 for epoch 17\n",
      "Val accuracy 17:\n",
      "Epoch 18/20 - Train loss: 0.08167378652724437, Validation loss: 0.4628206742927432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:01<00:00,  1.54s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 18\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 18\n",
      "Count of iterations: 10 for epoch 18\n",
      "Val accuracy 18:\n",
      "Epoch 19/20 - Train loss: 0.03611294325964991, Validation loss: 0.5136280605569482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:02<00:00,  1.55s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 19\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4f9536650>\n",
      "Label predict shape : 10 for epoch 19\n",
      "Count of iterations: 10 for epoch 19\n",
      "Val accuracy 19:\n",
      "Epoch 20/20 - Train loss: 0.024875717506802175, Validation loss: 0.5597770504653454\n",
      "Salvado das metricas de validacao e treino\n",
      "Pasta output/metricas/valid/fold_0 ja existe.\n",
      "Metricas salvas em output/metricas/valid/fold_0 com os nomes predict_label_valid_fold_0.npy e true_label_valid_fold_0.npy e tamanhos (20, 10, 10) e (20, 10, 10)\n",
      "Pasta output/metricas/train/fold_0 ja existe.\n",
      "Metricas salvas em output/metricas/train/fold_0 com os nomes predict_label_train_fold_0.npy e true_label_train_fold_0.npy e tamanhos (20, 40, 10) e (20, 40, 10)\n",
      "Finalizado o salvamento das metricas\n",
      "Fold 1:\n",
      "Pasta output/modelos ja existe.\n",
      "Train and valid for Fold 1\n",
      "Number of training images per iteration: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:03<00:00,  1.58s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 0\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 0\n",
      "Count of iterations: 10 for epoch 0\n",
      "Val accuracy 0:\n",
      "Epoch 1/20 - Train loss: 0.6209552764892579, Validation loss: 0.6854379594326019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:06<00:00,  1.67s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 1\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 1\n",
      "Count of iterations: 10 for epoch 1\n",
      "Val accuracy 1:\n",
      "Epoch 2/20 - Train loss: 0.33252842612564565, Validation loss: 0.5557798832654953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.65s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 2\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 2\n",
      "Count of iterations: 10 for epoch 2\n",
      "Val accuracy 2:\n",
      "Epoch 3/20 - Train loss: 0.22418878013268112, Validation loss: 0.7513783678412438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:06<00:00,  1.65s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 3\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 3\n",
      "Count of iterations: 10 for epoch 3\n",
      "Val accuracy 3:\n",
      "Epoch 4/20 - Train loss: 0.09652427835389972, Validation loss: 0.47077207565307616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:06<00:00,  1.66s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 4\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 4\n",
      "Count of iterations: 10 for epoch 4\n",
      "Val accuracy 4:\n",
      "Epoch 5/20 - Train loss: 0.07976440746570006, Validation loss: 0.5681142896413803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:07<00:00,  1.68s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 5\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 5\n",
      "Count of iterations: 10 for epoch 5\n",
      "Val accuracy 5:\n",
      "Epoch 6/20 - Train loss: 0.1984153866302222, Validation loss: 1.5051294267177582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.64s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 6\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 6\n",
      "Count of iterations: 10 for epoch 6\n",
      "Val accuracy 6:\n",
      "Epoch 7/20 - Train loss: 0.11809288874501363, Validation loss: 1.1346247136592864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.64s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 7\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 7\n",
      "Count of iterations: 10 for epoch 7\n",
      "Val accuracy 7:\n",
      "Epoch 8/20 - Train loss: 0.11669749721186236, Validation loss: 0.8818129643797874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:06<00:00,  1.65s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 8\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 8\n",
      "Count of iterations: 10 for epoch 8\n",
      "Val accuracy 8:\n",
      "Epoch 9/20 - Train loss: 0.10925816134549678, Validation loss: 0.7834086388349533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.63s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 9\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 9\n",
      "Count of iterations: 10 for epoch 9\n",
      "Val accuracy 9:\n",
      "Epoch 10/20 - Train loss: 0.09430404508020729, Validation loss: 0.6449060723185539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.65s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 10\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 10\n",
      "Count of iterations: 10 for epoch 10\n",
      "Val accuracy 10:\n",
      "Epoch 11/20 - Train loss: 0.08252326825167984, Validation loss: 0.5261526353657245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.64s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 11\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 11\n",
      "Count of iterations: 10 for epoch 11\n",
      "Val accuracy 11:\n",
      "Epoch 12/20 - Train loss: 0.059616687113884834, Validation loss: 0.7090119481086731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:06<00:00,  1.66s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 12\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 12\n",
      "Count of iterations: 10 for epoch 12\n",
      "Val accuracy 12:\n",
      "Epoch 13/20 - Train loss: 0.09235975123010576, Validation loss: 0.6385903431102633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:06<00:00,  1.66s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 13\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 13\n",
      "Count of iterations: 10 for epoch 13\n",
      "Val accuracy 13:\n",
      "Epoch 14/20 - Train loss: 0.04613861157558859, Validation loss: 0.5859310440719128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:06<00:00,  1.65s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 14\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 14\n",
      "Count of iterations: 10 for epoch 14\n",
      "Val accuracy 14:\n",
      "Epoch 15/20 - Train loss: 0.05150132927519735, Validation loss: 0.7325790509581566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:06<00:00,  1.65s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 15\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 15\n",
      "Count of iterations: 10 for epoch 15\n",
      "Val accuracy 15:\n",
      "Epoch 16/20 - Train loss: 0.03699542532558553, Validation loss: 0.8025525329168886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:06<00:00,  1.67s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 16\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 16\n",
      "Count of iterations: 10 for epoch 16\n",
      "Val accuracy 16:\n",
      "Epoch 17/20 - Train loss: 0.09257306091021747, Validation loss: 0.7067226227372885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:06<00:00,  1.65s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 17\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 17\n",
      "Count of iterations: 10 for epoch 17\n",
      "Val accuracy 17:\n",
      "Epoch 18/20 - Train loss: 0.06780485596391372, Validation loss: 0.5635670138522982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.64s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 18\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 18\n",
      "Count of iterations: 10 for epoch 18\n",
      "Val accuracy 18:\n",
      "Epoch 19/20 - Train loss: 0.07037575648573693, Validation loss: 0.5608493149280548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.63s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 19\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4eebccef0>\n",
      "Label predict shape : 10 for epoch 19\n",
      "Count of iterations: 10 for epoch 19\n",
      "Val accuracy 19:\n",
      "Epoch 20/20 - Train loss: 0.026840622059535236, Validation loss: 0.8041184782981873\n",
      "Salvado das metricas de validacao e treino\n",
      "Pasta output/metricas/valid/fold_1 ja existe.\n",
      "Metricas salvas em output/metricas/valid/fold_1 com os nomes predict_label_valid_fold_1.npy e true_label_valid_fold_1.npy e tamanhos (20, 10, 10) e (20, 10, 10)\n",
      "Pasta output/metricas/train/fold_1 ja existe.\n",
      "Metricas salvas em output/metricas/train/fold_1 com os nomes predict_label_train_fold_1.npy e true_label_train_fold_1.npy e tamanhos (20, 40, 10) e (20, 40, 10)\n",
      "Finalizado o salvamento das metricas\n",
      "Fold 2:\n",
      "Pasta output/modelos ja existe.\n",
      "Train and valid for Fold 2\n",
      "Number of training images per iteration: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:06<00:00,  1.65s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 0\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 0\n",
      "Count of iterations: 10 for epoch 0\n",
      "Val accuracy 0:\n",
      "Epoch 1/20 - Train loss: 0.6109098479151726, Validation loss: 0.49030978977680206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:06<00:00,  1.66s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 1\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 1\n",
      "Count of iterations: 10 for epoch 1\n",
      "Val accuracy 1:\n",
      "Epoch 2/20 - Train loss: 0.3603760164231062, Validation loss: 0.4627368912100792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.64s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 2\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 2\n",
      "Count of iterations: 10 for epoch 2\n",
      "Val accuracy 2:\n",
      "Epoch 3/20 - Train loss: 0.21005564099177718, Validation loss: 0.6986100852489472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.63s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 3\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 3\n",
      "Count of iterations: 10 for epoch 3\n",
      "Val accuracy 3:\n",
      "Epoch 4/20 - Train loss: 0.15529567962512375, Validation loss: 0.9981987059116364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.64s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 4\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 4\n",
      "Count of iterations: 10 for epoch 4\n",
      "Val accuracy 4:\n",
      "Epoch 5/20 - Train loss: 0.167246906994842, Validation loss: 0.5803611010313035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.64s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 5\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 5\n",
      "Count of iterations: 10 for epoch 5\n",
      "Val accuracy 5:\n",
      "Epoch 6/20 - Train loss: 0.15517274644225837, Validation loss: 0.46312612295150757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:06<00:00,  1.65s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 6\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 6\n",
      "Count of iterations: 10 for epoch 6\n",
      "Val accuracy 6:\n",
      "Epoch 7/20 - Train loss: 0.15785675516817718, Validation loss: 0.9535885334014893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.64s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 7\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 7\n",
      "Count of iterations: 10 for epoch 7\n",
      "Val accuracy 7:\n",
      "Epoch 8/20 - Train loss: 0.09008279934059829, Validation loss: 0.5231178224086761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.64s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 8\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 8\n",
      "Count of iterations: 10 for epoch 8\n",
      "Val accuracy 8:\n",
      "Epoch 9/20 - Train loss: 0.06374903805553914, Validation loss: 0.611599975079298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.65s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 9\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 9\n",
      "Count of iterations: 10 for epoch 9\n",
      "Val accuracy 9:\n",
      "Epoch 10/20 - Train loss: 0.10750417839735746, Validation loss: 0.5999389171600342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.65s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 10\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 10\n",
      "Count of iterations: 10 for epoch 10\n",
      "Val accuracy 10:\n",
      "Epoch 11/20 - Train loss: 0.059472394839394836, Validation loss: 0.6573609948158264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.64s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 11\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 11\n",
      "Count of iterations: 10 for epoch 11\n",
      "Val accuracy 11:\n",
      "Epoch 12/20 - Train loss: 0.07745183563965838, Validation loss: 0.6349450526759028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.63s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 12\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 12\n",
      "Count of iterations: 10 for epoch 12\n",
      "Val accuracy 12:\n",
      "Epoch 13/20 - Train loss: 0.06055542229150888, Validation loss: 0.8247234217822552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.64s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 13\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 13\n",
      "Count of iterations: 10 for epoch 13\n",
      "Val accuracy 13:\n",
      "Epoch 14/20 - Train loss: 0.04629378519020975, Validation loss: 0.8047408536076546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.64s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 14\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 14\n",
      "Count of iterations: 10 for epoch 14\n",
      "Val accuracy 14:\n",
      "Epoch 15/20 - Train loss: 0.13127363114617766, Validation loss: 0.521865026652813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:06<00:00,  1.65s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 15\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 15\n",
      "Count of iterations: 10 for epoch 15\n",
      "Val accuracy 15:\n",
      "Epoch 16/20 - Train loss: 0.10456441931892187, Validation loss: 0.7809827940538525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.64s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 16\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 16\n",
      "Count of iterations: 10 for epoch 16\n",
      "Val accuracy 16:\n",
      "Epoch 17/20 - Train loss: 0.04286162058124319, Validation loss: 0.6568442532792688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.63s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 17\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 17\n",
      "Count of iterations: 10 for epoch 17\n",
      "Val accuracy 17:\n",
      "Epoch 18/20 - Train loss: 0.0555847886542324, Validation loss: 0.6418427899479866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:06<00:00,  1.67s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:05<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 18\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 18\n",
      "Count of iterations: 10 for epoch 18\n",
      "Val accuracy 18:\n",
      "Epoch 19/20 - Train loss: 0.024346419825451448, Validation loss: 0.7416035577654838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:05<00:00,  1.63s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 19\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee6439b0>\n",
      "Label predict shape : 10 for epoch 19\n",
      "Count of iterations: 10 for epoch 19\n",
      "Val accuracy 19:\n",
      "Epoch 20/20 - Train loss: 0.06037598201146466, Validation loss: 0.7965994425117969\n",
      "Salvado das metricas de validacao e treino\n",
      "Pasta output/metricas/valid/fold_2 ja existe.\n",
      "Metricas salvas em output/metricas/valid/fold_2 com os nomes predict_label_valid_fold_2.npy e true_label_valid_fold_2.npy e tamanhos (20, 10, 10) e (20, 10, 10)\n",
      "Pasta output/metricas/train/fold_2 ja existe.\n",
      "Metricas salvas em output/metricas/train/fold_2 com os nomes predict_label_train_fold_2.npy e true_label_train_fold_2.npy e tamanhos (20, 40, 10) e (20, 40, 10)\n",
      "Finalizado o salvamento das metricas\n",
      "Fold 3:\n",
      "Pasta output/modelos ja existe.\n",
      "Train and valid for Fold 3\n",
      "Number of training images per iteration: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop: 100%|██████████| 40/40 [01:04<00:00,  1.60s/it]\n",
      "Validation loop: 100%|██████████| 10/10 [00:04<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End validation for epoch 0\n",
      "Amount of images validated: <torch.utils.data.dataloader.DataLoader object at 0x75f4ee7fd150>\n",
      "Label predict shape : 10 for epoch 0\n",
      "Count of iterations: 10 for epoch 0\n",
      "Val accuracy 0:\n",
      "Epoch 1/20 - Train loss: 0.6142537228763103, Validation loss: 0.6568727850914001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loop:  28%|██▊       | 11/40 [00:16<00:43,  1.51s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m     salvar_model(model, path=\u001b[33m'\u001b[39m\u001b[33moutput/modelos\u001b[39m\u001b[33m'\u001b[39m, name_file=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmodel_fold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTrain and valid for Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m t, l,_,outputs,labels = \u001b[43msimple_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_index\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m## Evaluate model.\u001b[39;00m\n\u001b[32m     27\u001b[39m train_loss_total.append(t)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36msimple_loop\u001b[39m\u001b[34m(model, train_image, val_image, epochs, batch_size, fold_index)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m#loss_train = criterion(outputs.float(), label.float())\u001b[39;00m\n\u001b[32m     56\u001b[39m loss_train = criterion(outputs, label)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43mloss_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m optimizer.step()\n\u001b[32m     59\u001b[39m running_loss_train += loss_train.item() * label.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/oca-ia/.venv/lib/python3.13/site-packages/torch/_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/oca-ia/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/oca-ia/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "## create first model.\n",
    "print('''\n",
    "      ###############################################\n",
    "      Inicio do treinamento com K-Fold Cross Validation\n",
    "      ###############################################\n",
    "      ''')\n",
    "device = 'cuda:0'\n",
    "    \n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train_dataset)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    #print(f\"  Train: index={train_index}\")\n",
    "    #print(f\"  Test:  index={test_index}\")\n",
    "    ## init train test for folder\n",
    "    train_dataset_part = Subset( train_dataset, train_index)\n",
    "    val_dataset_part = Subset( train_dataset, test_index)\n",
    "\n",
    "    train_loader_img = DataLoader(train_dataset_part, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader_img = DataLoader(val_dataset_part, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    model= ECGClassifierResnet( num_classes=1)\n",
    "    if (flg_salvar_modelos):\n",
    "        salvar_model(model, path='output/modelos', name_file=f'model_fold_{i}.pth')\n",
    "    print(f'Train and valid for Fold {i}')\n",
    "    t, l,_,outputs,labels = simple_loop(model, train_loader_img,val_loader_img, epochs, batch_size = BATCH_SIZE, fold_index =i)\n",
    "    ## Evaluate model.\n",
    "    train_loss_total.append(t)\n",
    "    val_loss_total.append(l)\n",
    "\n",
    "print('''\n",
    "      ###############################################\n",
    "      Fim do treinamento com K-Fold Cross Validation\n",
    "      ###############################################\n",
    "      ''')\n",
    "print(\"Treinamento com K-Fold Cross Validation concluído com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345f4f10",
   "metadata": {},
   "source": [
    "## Métricas de Avaliacao dos modelos\n",
    "\n",
    "Acuracia\n",
    "\n",
    "Precisao\n",
    "\n",
    "Sensibilidade\n",
    "\n",
    "Especificidade\n",
    "\n",
    "F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70a0dd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Leitura de métricas salvas\n",
    "import numpy as np\n",
    "predict_label_0 = np.load('output/metricas/train/fold_0/predict_label_train_fold_0.npy')\n",
    "true_label_0 = np.load('output/metricas/train/fold_0/true_label_train_fold_0.npy')\n",
    "# --- IGNORE ---\n",
    "#print(true_label_0[1][0][0])\n",
    "criterio = nn.BCEWithLogitsLoss()\n",
    "len(predict_label_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ce677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 1],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 1, 0, 1, 1],\n",
       "        [1, 1, 0, 0, 1],\n",
       "        [1, 1, 0, 0, 0],\n",
       "        [1, 0, 1, 0, 0],\n",
       "        [0, 1, 1, 0, 1],\n",
       "        [1, 0, 1, 1, 0]],\n",
       "\n",
       "       [[0, 1, 0, 1, 1],\n",
       "        [0, 1, 1, 1, 0],\n",
       "        [1, 1, 0, 0, 1],\n",
       "        [1, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 0],\n",
       "        [1, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 1],\n",
       "        [0, 1, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 0],\n",
       "        [0, 1, 0, 0, 1]],\n",
       "\n",
       "       [[0, 0, 0, 1, 1],\n",
       "        [1, 0, 0, 0, 1],\n",
       "        [0, 1, 1, 0, 1],\n",
       "        [1, 1, 0, 1, 1],\n",
       "        [1, 1, 0, 1, 0],\n",
       "        [1, 1, 1, 0, 1],\n",
       "        [1, 1, 0, 0, 0],\n",
       "        [1, 0, 1, 1, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 1]],\n",
       "\n",
       "       [[1, 0, 0, 0, 1],\n",
       "        [1, 0, 0, 1, 0],\n",
       "        [1, 1, 1, 0, 0],\n",
       "        [0, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [0, 1, 0, 1, 1],\n",
       "        [1, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 1],\n",
       "        [0, 1, 1, 1, 1],\n",
       "        [0, 1, 0, 0, 1]],\n",
       "\n",
       "       [[1, 1, 0, 1, 1],\n",
       "        [1, 0, 0, 1, 0],\n",
       "        [0, 1, 1, 0, 1],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [0, 1, 0, 1, 0],\n",
       "        [1, 0, 0, 1, 0],\n",
       "        [0, 1, 1, 1, 1],\n",
       "        [0, 1, 1, 0, 1]]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_label_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e2df49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]]\n",
      "\n",
      " [[ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]]\n",
      "\n",
      " [[ 0  0  0  0  1]\n",
      "  [ 0  0  0  0  1]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 1  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 1  0  0  0  0]\n",
      "  [ 0  1  0  0  1]]\n",
      "\n",
      " [[ 1  0  0  0  0]\n",
      "  [ 1  0  0  1  0]\n",
      "  [ 0  1  1 -1 -1]\n",
      "  [-1  1  0  0  0]\n",
      "  [ 0  1  0  0 -1]\n",
      "  [-1  1 -1  1  1]\n",
      "  [ 1 -1 -1  0  2]\n",
      "  [ 0  0  0  0  2]\n",
      "  [-1  1  0  0  1]\n",
      "  [-1  1  0  0  1]]\n",
      "\n",
      " [[ 1  0 -2  1  1]\n",
      "  [ 2 -1 -1  3 -1]\n",
      "  [-2  2  2 -2  2]\n",
      "  [-1  0  0  3  0]\n",
      "  [-1  0  0  3  0]\n",
      "  [ 1  1  0  2 -3]\n",
      "  [-1  2 -1  3 -1]\n",
      "  [ 4 -2 -1  3 -2]\n",
      "  [-2  0  1  2  1]\n",
      "  [-1  2  2 -2  0]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predict_label_0)\n",
    "len(predict_label_0[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1251295b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch 0: 0.6931471228599548\n",
      "Loss for batch 1: 0.6931471228599548\n",
      "Loss for batch 2: 0.6475609540939331\n",
      "Loss for batch 3: 0.4805556833744049\n",
      "Loss for batch 4: 0.2919488847255707\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(predict_label_0)):\n",
    "    outputs_tensor = torch.tensor(predict_label_0[i]).to(torch.float)\n",
    "    labels_tensor = torch.tensor(true_label_0[i]).to(torch.float)\n",
    "    loss = criterio(outputs_tensor, labels_tensor)\n",
    "    print(f'Loss for batch {i}: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd73f641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_label_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9399a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2821440 2821440 2821440 2821440 2821440]\n",
      "  [3546063 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 4119212]\n",
      "  [2821440 2821440 3188174 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2471002]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [3436914 2821440 2821440 2821440 3913112]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 3827248 2821440 2821440]\n",
      "  [2821440 2821440 2631008 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2514506 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2847649 2821440 3869186 2821440]\n",
      "  [2759107 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 2821440]\n",
      "  [2821440 2821440 2821440 2821440 3113366]\n",
      "  [2821440 2821440 2821440 2821440 3379505]]\n",
      "\n",
      " [[    564     564    1204     564     564]\n",
      "  [    564     564     564     564     539]\n",
      "  [    564     564     564     564     564]\n",
      "  [    564     394     564     564     564]\n",
      "  [    564     564    1160     564     564]\n",
      "  [    564     564     564     564     564]\n",
      "  [    564     564     564     508     564]\n",
      "  [    564     264     564     564     564]\n",
      "  [    564     564     564     564    1093]\n",
      "  [    564     564    1578     564     564]\n",
      "  [    564     564     564     564     564]\n",
      "  [    564     564    2208     564     564]\n",
      "  [    564     564     564    1628     564]\n",
      "  [    564     564     564    1815     564]\n",
      "  [    564     564     564     564     564]\n",
      "  [    666     564     564     564     564]\n",
      "  [    564     564     564     564     564]\n",
      "  [    564     564     564     564     564]\n",
      "  [    564     564     564     564    2193]\n",
      "  [    564     564     564     564     564]\n",
      "  [    564     564     564     169     676]]]\n",
      "[[0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1]\n",
      " [1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#### Leitura de métricas salvas\n",
    "import numpy as np\n",
    "predict_label_0_valid= np.load('/Users/leonardocipriani/Documents/dev/python/Artificial Intelligence Projects/oca-ia/incor_env/output/metricas/valid/fold_0/predict_label_valid_fold_0.npy')\n",
    "true_label_0_valid= np.load('/Users/leonardocipriani/Documents/dev/python/Artificial Intelligence Projects/oca-ia/incor_env/output/metricas/valid/fold_0/true_label_valid_fold_0.npy')\n",
    "# --- IGNORE ---\n",
    "print(predict_label_0_valid)\n",
    "print(true_label_0_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc67921",
   "metadata": {},
   "source": [
    "## Criacao graficos de treinamento e validacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f25929",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Criar graficos de treinamento e validacao\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(5):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(train_loss_total[i], label='Train Loss')\n",
    "    plt.plot(val_loss_total[i], label='Validation Loss')\n",
    "    plt.title(f'Fold {i} - Train and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'output/graficos/loss_fold_{i}.png')\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fdd0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source incor_env/Scripts/activate\n"
     ]
    }
   ],
   "source": [
    "activate_script = os.path.join(\"incor_env\", \"Scripts\", \"activate\")\n",
    "if os.name == 'nt':  # Windows\n",
    "    activate_command = f\"{activate_script}\"\n",
    "else:  # macOS/Linux\n",
    "    activate_command = f\"source {activate_script}\"\n",
    "print(activate_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86485574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando ambiente virtual...\n",
      "total 1680\n",
      "drwxr-xr-x@ 19 leonardocipriani  staff     608 Dec 14 10:14 \u001b[34m.\u001b[m\u001b[m\n",
      "drwxr-xr-x@ 16 leonardocipriani  staff     512 Nov 25 13:15 \u001b[34m..\u001b[m\u001b[m\n",
      "drwxr-xr-x@ 16 leonardocipriani  staff     512 Dec 14 10:01 \u001b[34m.git\u001b[m\u001b[m\n",
      "-rw-r--r--   1 leonardocipriani  staff      13 Sep 13 17:55 .gitignore\n",
      "drwxr-xr-x   7 leonardocipriani  staff     224 Dec 14 10:14 \u001b[34m.incor_env\u001b[m\u001b[m\n",
      "drwxr-xr-x   4 leonardocipriani  staff     128 Dec 13 16:10 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\n",
      "drwxr-xr-x   9 leonardocipriani  staff     288 Oct 14 20:26 \u001b[34m.venv\u001b[m\u001b[m\n",
      "-rw-r--r--   1 leonardocipriani  staff    1967 Dec 11 07:20 README.md\n",
      "-rw-r--r--   1 leonardocipriani  staff  683671 Dec  7 10:06 classificador_ml.ipynb\n",
      "-rw-r--r--   1 leonardocipriani  staff   70005 Dec 14 10:15 classificador_oca copy.ipynb\n",
      "-rw-r--r--   1 leonardocipriani  staff   84135 Dec 13 16:18 classificador_oca.ipynb\n",
      "drwxr-xr-x   9 leonardocipriani  staff     288 Dec 13 16:18 \u001b[34mdataset\u001b[m\u001b[m\n",
      "-rw-r--r--@  1 leonardocipriani  staff     632 Sep  3 16:31 exemplo_csv.csv\n",
      "drwxr-xr-x   3 leonardocipriani  staff      96 Sep 28 18:46 \u001b[34mimgs\u001b[m\u001b[m\n",
      "drwxr-xr-x   9 leonardocipriani  staff     288 Dec 13 18:58 \u001b[34mincor_env\u001b[m\u001b[m\n",
      "drwxr-xr-x   4 leonardocipriani  staff     128 Dec  7 10:06 \u001b[34mlib\u001b[m\u001b[m\n",
      "drwxr-xr-x   3 leonardocipriani  staff      96 Sep  9 01:39 \u001b[34mmodels\u001b[m\u001b[m\n",
      "drwxr-xr-x   5 leonardocipriani  staff     160 Dec  7 10:06 \u001b[34moutput\u001b[m\u001b[m\n",
      "-rw-r--r--   1 leonardocipriani  staff     133 Dec  7 10:06 requirements.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def config_venv():\n",
    "    print(\"Criando ambiente virtual...\")\n",
    "    os.system(\"ls -la\")    \n",
    "    os.system(\"python -m venv .incor_env\")\n",
    "\n",
    "config_venv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870787b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ds_file = pd.read_csv('dataset/oca_incor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_file.rename(columns={ds_file.columns[0]:'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2e372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image1.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image2.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image3.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image4.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image5.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>image16.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>image16.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>image16.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>image16.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>image16.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           label  label\n",
       "0     image1.png      0\n",
       "1     image2.png      0\n",
       "2     image3.png      1\n",
       "3     image4.png      1\n",
       "4     image5.png      1\n",
       "..           ...    ...\n",
       "211  image16.png      1\n",
       "212  image16.png      0\n",
       "213  image16.png      0\n",
       "214  image16.png      1\n",
       "215  image16.png      1\n",
       "\n",
       "[216 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488fbc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
